\chapter{Discussion} \label{sec:discussion}

In this thesis we set out to inspect the possible benefits of pre-training on \gls{lstm} and transformer encoder models to answer the questions:

\begin{itemize}
	\item R1: Can self-supervised pre-training improve the flow classification capabilities of an \gls{lstm} model?
	\item R2: Can self-supervised pre-training improve the flow classification capabilities of a transformer encoder model?
	\item R3: Which pre-training tasks improve accuracy and which do not?
	\item R4: If improvement is possible, how can it be explained?
\end{itemize}

The results of our experiments appear to be inconclusive. Some experiments have shown minor improvements, but later experiments with the same proxy task and less data have shown either no improvement or worse performance. As can be seen in tables \ref{table:discussion:lstm:accuracy_differences} and \ref{table:discussion:transformer:accuracy_differences}, the \gls{lstm} model seems to be more receptive for pre-training but no clear pattern emerges which could point at a proxy-task that generally improves accuracy when used in pre-training. The first and second research questions, R1 and R2, must therefore be answered with "it depends":

\begin{itemize}
	\item A1: Yes, pre-training can improve flow classification accuracy of \gls{lstm} models in some instances, but it can not be concluded that this is generally so.
\end{itemize}

Referring to section \ref{table:discussion:transformer:accuracy_differences} it can be seen that pre-training seems to have even less positive impact on the transformer model compared to the \gls{lstm} model in general, but still yielded some positive results for experiments with very little supervised data i.e. the dedicated subsets used for experiments 3.3.3 and 3.6.4 as can be seen in tables \ref{table:results:transformer:stats_flows_subset} and \ref{table:results:transformer:stats_flows15_subset}.

\begin{itemize}
	\item A2: Yes, pre-training can improve flow classification accuracy of transformer encoder models in very specific instances, but in most cases, it does not.
\end{itemize}

We could name the predict packet (PREDICT) and surprisingly the identity function (ID) proxy task as most likely to improve final accuracy of the \gls{lstm} model. In table \ref{table:discussion:lstm:accuracy_differences} we can see that the highest overall absolute increase in accuracy of 0.594\% was also achieved with pre-training with the ID proxy task in experiment 2.3.5 \ref{table:results:lstm:stats_flows_subset}. \par

\begin{itemize}
	\item A3: The \textit{predict packet (PREDICT)} and \textit{identity function (ID)} proxy tasks where most effective in increasing flow classification accuracy for \gls{lstm} models in our experiments. For transformer encoder models it was the \textit{auto encoder (AUTO)} proxy task with 20\% dropout rate.
\end{itemize}

As is often the case, complete explainability for \glspl{nn} is hard to achieve. From the \gls{pd} plots we can deduce that pre-training had a significant impact on the internal behavior of the models. The differences in neuron activations after pre-training with different proxy tasks suggests the same thing. We can therefore conclude that the internal representation of data is influenced in a positive way in the instances where pre-training improved classification accuracy when compared to a model with randomly initialized weights.

\begin{itemize}
	\item A4: Pre-training can be seen as a non-random weight initialization before conducting supervised training which leads to an alternate internal data representation in the hidden state and ultimately to an increase in flow classification accuracy.
\end{itemize}

The fact that any improvement was observable at all is already promising, when considering that the baseline results of exclusively supervised trained models are already very high when compared to other state-of-the-art results for the used datasets in contemporary research. Increasing accuracy ever so slightly without the need for more labeled data might make an \gls{ml} based \gls{ids} feasible in a real world scenario when before it was not. There could be several reasons why no conclusive pattern can be found in the results: \par

\begin{table}[]
	\centering
	\scalebox{0.9}{
		\begin{tabular}{rccccc}
			\thead{Experiments (\#)} & \thead{PREDICT(2)} & \thead{OBSCURE(3)} & \thead{AUTO(4)}  & \thead{ID(5)}            & \thead{COMPOSITE(6)} \\ \midrule
			\multicolumn{6}{c}{CIC-IDS2017} \\ \midrule
			10\% (2.1.x)        & 0.073\%    & -0.012\%   & -0.002\% & 0.095\%          & \textbf{0.101\%}      \\
			1\% (2.2.x)         & \textbf{0.178\%}    & -0.075\%   & -0.007\% & 0.094\%          & 0.136\%      \\
			CIC2017\_10 (2.3.x) & -1.609\%   & -2.327\%   & -0.744\% & \textbf{0.594\%} & -1.453\%     \\ \midrule
			\multicolumn{6}{c}{UNSW-NB15} \\ \midrule
			10\% (2.4.x)        & 0.083\%    & 0.067\%    & 0.045\%  & \textbf{0.086\%}          & 0.037\%      \\
			1\% (2.5.x)         & 0.108\%    & 0.043\%    & \textbf{0.137\%}  & 0.099\%          & 0.024\%      \\
			UNSW15\_10 (2.6.x)  & 0.005\%    & -0.115\%   & \textbf{0.010\%}  & -0.089\%         & -0.168\%     \\ \midrule
			\makecell{Max. absolute \\ accuracy increase} & 0.178\%    & 0.067\%    & 0.137\%  & \textbf{0.594\%}          & 0.136\%      \\
		\end{tabular}
	}
	\caption{Absolute differences of validation accuracy between differently pre-trained \gls{lstm} model and the same model without pre-training. The highest value of each row is marked bold.}
	\label{table:discussion:lstm:accuracy_differences}
\end{table}

\begin{table}[]
	\centering
	\resizebox{0.7\linewidth}{!}{\begin{tabular}{rlll}
		\thead{Experiments (\#)} & \thead{MASK(2)} & \thead{OBSCURE(3)} & \thead{AUTO(4)}  \\ \midrule
		\multicolumn{4}{c}{CIC-IDS2017} 											 \\ \midrule
		10\% (3.1.x)        & \textbf{-0.037\%} & -0.724\%          & -0.135\%         \\
		1\% (3.2.x)         & -0.084\%         & \textbf{-0.021\%}         & -0.098\%         \\
		CIC2017\_10 (3.3.x) & -1.055\%         & \textbf{0.678\%} 	& -0.756\%         \\ \midrule
		\multicolumn{4}{c}{UNSW-NB15} 												 \\ \midrule
		10\% (3.4.x)        & -1.817\%         & -0.361\%         & \textbf{-0.018\%}         \\
		1\% (3.5.x)         & -0.163\%         & -0.072\%         & \textbf{0.003\%}          \\
		UNSW15\_10 (3.6.x)  & -1.213\%         & -0.228\%         & \textbf{0.295\%} \\ \midrule
		\makecell{Max. absolute \\ accuracy increase}    & 0.037\%          & \textbf{0.678\%} & 0.295\%          \\
	\end{tabular}}
	\caption{Absolute differences of validation accuracy between differently pre-trained transformer model and the same model without pre-training. The highest value of each row is marked bold.}
	\label{table:discussion:transformer:accuracy_differences}
\end{table}


The achieved accuracy scores by the models when fine-tuned with 90\% of data were in the high 99.x percent \ref{table:results:explainability:model_comparison} and even with only one percent of data used for training, accuracy was still over 99\%. The flow representation used omits the packet payload, so payload based attacks like \textit{SQL Injection} or \textit{XSS} attacks are very hard, if not impossible, to detect. There might simply be very little room for improvement. So even if pre-training were effective, it would not show up in our results. One approach to mitigate this effect would be to switch to multinomial classification of the exact attack type instead of binary classification. This approach increases fine-tuning training time, but since 
unsupervised pre-training takes disproportionately longer than fine-tuning at the moment, overall training time would not increase significantly. \par

In accordance with the last point, the datasets we used seem to be easy to classify. Even though not completely comparable, the results from the \gls{dtc} alone show that records in both datasets can be classified with high accuracy, i.e. about 97\% or more \ref{table:results:explainability:model_comparison} even without access to flow information.
The lack of complexity of the data might be another reason why pre-training in this case showed no improvement as supervised training was always sufficient to extract all relevant information from the input data. To inspect this conjecture, a more challenging dataset should be used or a combination of different datasets. Ideal would be actual captured traffic of a mid size company during a penetration test. \par

Word embeddings are an integral part of modern \gls{nlp} systems. This aspect is not included in our process since our feature vectors were already given. As our feature representation contains some qualitative features which are mapped to a mostly arbitrary number e.g. the IP protocol identifier and port numbers, creating embeddings for qualitative features and tuning them to the task at hand might lead to improvement in overall accuracy, independent of whether pre-training is used or not. \par
	
Insufficient data or model complexity might be another reason why pre-training has produced no consistent improvement. Google BERT \cite{bert} uses a model with 110 million parameters in its \textit{BASE} version and 340 million in for its \textit{LARGE} version. In comparison, our \gls{lstm} model consists of 5.3 million parameters and the transformer encoder model only contains 74 thousand parameters. In the original paper about BERT, google claims to have pre-trained their model on a corpus of about 3.3 billion words. During our work on this thesis many new \gls{nlp} models like Googles T5 \cite{google_t5}, Nvidias Megatron \cite{megatron} were presented with the largest one being OpenAIs GPT-3 with 175 billion parameters, pre-trained on 410 billion tokens \cite{gpt3}. The datasets we are using contain around 25 million packets each which is two magnitudes smaller than the corpus Googles BERT was trained on. Larger models or significantly larger datasets would not have been feasible in our case due to lack of computational resources. One of these two reasons, i.e. either lack of model complexity or pre-training data, might be the cause of our inconclusive results. This hypotheses is of course not easily checked without the necessary resources. As already stated above, the dataset used for unsupervised pre-training could be magnified by merging multiple datasets together or even using available unlabeled network traffic data for pre-training. \par

Unsuited proxy tasks could also be a reason why pre-training showed little effect. The used proxy tasks appeared to us as most intuitive but other choices might have been more effective. There is also the possibility of pre-training using labeled data where e.g. a custom dataset is constructed where flows are labeled with the application layer protocol the flow is part of. Other unsupervised training methods might also be feasible e.g. energy-based unsupervised learning \cite{energy_based_training}. The possible approaches here are many and we have by no means explored all our options. \par

Looking at the validation and training loss progression during supervised fine-tuning, it shows that especially in experiments where models are trained on only 1\% of the dataset or even less, i.e. with the specialized subsets, the models start to overfit heavily. It might be that the effects of over-fitting in these scenarios mitigates any positive effect pre-training might have had. During fine-tuning the model learns two things: Patterns in the data based on the label, but also how to classify data at all. At the very least, the fully connected layer must be trained for classification in the fine-tuning phase. During pre-training the model might have constructed a perfectly useful latent feature space, but it has not yet learned how to do actual classification based on it. This poses the difficulty of teaching the model how to classify, but not overfit it on the little data it has, overwriting any possible gains from pre-training. Deducted from this hypothesis, it should be a requirement that the time the model takes to converge should be greater than the time the model learns to classify at all. This problem is implicitly mitigated by increasing the size of the dataset.\par

Furthermore, pre-training was performed with parts of the same dataset which was also used for supervised fine-tuning and for validation. Even though the labels where ignored, the data used for pre-training contained attack flows. If anything, this is most likely beneficial for possible positive effects on the final accuracy and metrics and has to be considered when trying to reproduce the results. This effect could be mitigated by e.g. using one dataset for pre-training and a different dataset for fine-tuning. In a real world setting however our scenario is more likely to apply as for pre-training a \gls{nids} it would make most sense to train the model with network traffic captured within the network the system is going to protect. This would also mean that it has to be assumed that the data also contains some attack flows as there is no way of asserting otherwise. For a real world application this approach also assures that the models learn common patterns of the specific networks they are going to be used in. It might even make sense to re-train or at least update the model periodically with recent data. Unfortunately network protocols are not as universal as natural language which makes transfer learning more difficult .i.e. it would be hard to teach the model universal patterns of network traffic. An ad-hoc approach tailored to the traffic of a single specific network is more likely to yield usable results in the near future. \par

An unsuited data representation might stifle the effectiveness of the used models. Even after deciding to use a flows representation, there is a wide selection of feature spaces \cite{feature_vectors}.
Our analysis of the datasets in section \ref{sec:results:explainability:dtc}, in particular table \ref{table:results:expl:fimp_90_ds}, shows that a lot of importance is accumulated in a few features while others are mostly irrelevant which shows that the selection of the correct feature and data representation has a significant effect on the performance of the models. Omitting unused or irrelevant features from the data representation would drastically decrease training time, but it is of course impossible to tell \textit{a priori} if a feature is going to be relevant for classification of the data at hand. There are however feature selection and feature reduction methods that can and should be applied. \par
We used a flow representation of per-packet feature vectors instead of the often used approach of aggregating all packets of a flow into a single feature vector containing statistical data. This was, as already explained in earlier sections, to enable the use of machine learning techniques used in \gls{nlp} which almost exclusively expect a sequences of tokens as input. When considering the immense overhead of using sequences instead of a single vector and looking at the results of the \gls{dtc} which has no concept of flows but still performs reasonably well, there might be better ways to build meaningful sequences. A possible approach is to use a sequence constituting of aggregated data over specified consecutive time frames comes to mind, which has already been used in other papers like \cite{attention_model_ids}, \cite{kitsune}. \par

\begin{table}[]
	\centering
	\begin{tabular}{rccccc}
		\thead{Experiment \#} & \thead{PREDICT(2)} & \thead{OBSCURE(3)} & \thead{AUTO(4)}   & \thead{ID(5)}      & \thead{COMPOSITE(6)} \\ \midrule
		\multicolumn{6}{c}{CIC-IDS2017} \\ \midrule
		10\% (2.1.x) 		& Yes    & No   & No   & Yes    & Yes      \\ 
		1\% (2.2.x) 		& Yes    & No   & No   & Yes    & Yes      \\ 
		CIC2017\_10 (2.3.x) 	& No   	 & No   & No   & Yes    & No     \\ \\ \midrule
		\multicolumn{6}{c}{UNSW-NB15} \\ \midrule
		10\% (2.4.x) 		& Yes    & Yes    & Yes    & Yes    & Yes      \\ 
		1\% (2.5.x) 		& Yes    & Yes    & Yes    & Yes    & Yes      \\ 
		UNSW15\_10 (2.6.x) 		& Yes    & No   & Yes    & No   & No     \\ \midrule
		\makecell{\# Cases in which \\ pre-training \\ improved accuracy} & 5/6 & 2/6 & 3/6 & 5/6 & 4/6  
	\end{tabular}
	\caption{Table of comparisons whether accuracy improved for pre-trained \gls{lstm} models when compared to supervised only trained baseline experiments.}
	\label{table:discussion:lstm:improvement_results}
\end{table}

\begin{table}[]
	\centering
	\begin{tabular}{rccc}
		\thead{Experiments (\#)} & \thead{MASK(2)} & \thead{OBSCURE(3)} & \thead{AUTO(4)} \\ \midrule
		\multicolumn{4}{c}{CIC-IDS2017} \\ \midrule
		10\% (3.1.x)   & No  & No   & No   \\
		1\% (3.2.x)    & No  & No   & No   \\
		CIC2017\_10 (3.3.x) & No  & Yes  & No   \\ \midrule
		\multicolumn{4}{c}{UNSW-NB15} \\ \midrule
		10\% (3.4.x)     & No  & No   & No   \\
		1\% (3.5.x)      & No  & No   & Yes  \\
		UNSW15\_10 (3.6.x)   & No  & No   & Yes  \\ \midrule
		\makecell{\# Cases in which \\ pre-training \\ improved accuracy} & 0/6 & 1/6 & 2/6
	\end{tabular}
	\caption{Table of comparisons whether accuracy improved for pre-trained transformer models when compared to supervised only trained baseline experiments.}
	\label{table:discussion:transformer:improvement_results}
\end{table}

While machine learning researchers in the realm of \gls{nlp} are already trying to move past the pre-training / fine-tuning paradigm \cite{gpt3}, it is yet to be explored in the context of \gls{nid}. We contributed by applying these established methods in an attempt to harness them for the very needed improvement of
machine learning based \gls{nid}. Although by no means exhaustive, we managed to achieve a useful primer by using both state-of-the-art models and techniques combined with careful inspection of the obtained results and the used data. The tables \ref{table:discussion:lstm:improvement_results} and \ref{table:discussion:transformer:improvement_results} serve as a final overview on the instances in which pre-training actually improved classification accuracy. Considering the difficulties and shortcomings stated above, the fact that it was possible to improve classification accuracy in some instances remains an encouraging pointer towards the feasibility of the approach overall. Taking into account that the datasets used seem to be a mismatch for our experiments and the fact that results from experiments with the specialized subsets should be ignored due to the negative effects of over-fitting, the results for the \gls{lstm} model become a lot more promising. 
The facts that improvements where only minor and that our experiments where quite narrow in the sense that we only performed them on two datasets and with the same random seed, remain. For these reasons further inquiry is needed, applying the lessons we learned during our research, to confirm that the results are generalizable for sequence2sequence models in the context of deep learning based \gls{nid}. \par

\newpage