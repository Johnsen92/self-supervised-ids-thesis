\chapter{Discussion} \label{sec:discussion}

In this thesis we set out to inspect the possible benefits of pre-training on \glspl{dnn} models and to answer the questions:

\begin{itemize}
	\item R1: Can self-supervised pre-training improve the flow classification capabilities of an \gls{lstm} model?
	\item R2: Can self-supervised pre-training improve the flow classification capabilities of a Transformer-Encoder model?
	\item R3: Which pre-training tasks improve accuracy and which do not?
	\item R4: If improvement is possible, how can it be explained?
\end{itemize}

The results of our experiments appear to be inconclusive. Some experiments have shown minor improvements, but later experiments with the same proxy task and less data have shown either no improvement or worse performance. As can be seen in tables \ref{table:discussion:lstm:improvement_results} and \ref{table:discussion:transformer:improvement_results}, the \gls{lstm} model seems to be more receptive for pre-training but no clear pattern emerges which could point at a proxy-task that generally improves accuracy when used in pre-training. We could name the predict packet (PREDICT) and surprisingly the identity function (ID) proxy task as most likely to improve final accuracy of the model. Pre-training seems to have even less impact on the transformer model compared to the \gls{lstm} model in general, but still yielded some positive results for experiments with very little supervised data i.e. the dedicated subsets used for experiments 3.3.3 and 3.6.4 as can be seen in tables \ref{table:results:transformer:stats_flows_subset} and \ref{table:results:transformer:stats_flows15_subset}. The fact that any improvement was observable at all is already promising, when considering that the baseline results of exclusively supervised trained models are already very high when compared to other state-of-the-art results in contemporary research. Increasing accuracy ever so slightly without the need for more labeled data might make a \gls{ml} based \gls{ids} feasible in a real world scenario when before it was not. There could be several reasons why no conclusive pattern can be found in the results: \par

\begin{table}[!h]
	\centering
	\begin{tabular}{cccccc}
		\thead{\textbf{Experiment \#}} & \thead{\textbf{PREDICT(2)}} & \thead{\textbf{OBSCURE(3)}} & \thead{\textbf{AUTO(4)}}   & \thead{\textbf{ID(5)}}      & \thead{\textbf{COMPOSITE(6)}} \\ \midrule
		\multicolumn{6}{l}{CIC-IDS2017} \\ \midrule
		10\% (2.1.x) 		& Yes    & No   & No   & Yes    & Yes      \\ 
		1\% (2.2.x) 		& Yes    & No   & No   & Yes    & Yes      \\ 
		CIC2017\_10 (2.3.x) 	& No   	 & No   & No   & Yes    & No     \\ \\ \midrule
		\multicolumn{6}{l}{UNSW-NB15} \\ \midrule
		10\% (2.4.x) 		& Yes    & Yes    & Yes    & Yes    & Yes      \\ 
		1\% (2.5.x) 		& Yes    & Yes    & Yes    & Yes    & Yes      \\ 
		UNSW15\_10 (2.6.x) 		& Yes    & No   & Yes    & No   & No     \\ \midrule
		\makecell{\# Cases in which \\ pre-training \\ improved accuracy} & 5/6 & 2/6 & 3/6 & 5/6 & 4/6  
	\end{tabular}
	\caption{Table of comparisons whether accuracy improved for pre-trained \gls{lstm} models when compared to supervised only trained baseline experiments.}
	\label{table:discussion:lstm:improvement_results}
\end{table}

\begin{table}[!h]
	\centering
	\begin{tabular}{rccc}
		\thead{\textbf{Experiments (\#)}} & \thead{\textbf{MASK(2)}} & \thead{\textbf{OBSCURE(3)}} & \thead{\textbf{AUTO(4)}} \\ \midrule
		\multicolumn{4}{l}{CIC-IDS2017} \\ \midrule
		10\% (3.1.x)   & No  & No   & No   \\
		1\% (3.2.x)    & No  & No   & No   \\
		CIC2017\_10 (3.3.x) & No  & Yes  & No   \\ \midrule
		\multicolumn{4}{l}{UNSW-NB15} \\ \midrule
		10\% (3.4.x)     & No  & No   & No   \\
		1\% (3.5.x)      & No  & No   & Yes  \\
		UNSW15\_10 (3.6.x)   & No  & No   & Yes  \\ \midrule
		\makecell{\# Cases in which \\ pre-training \\ improved accuracy} & 0/6 & 1/6 & 2/6
	\end{tabular}
	\caption{Table of comparisons whether accuracy improved for pre-trained transformer models when compared to supervised only trained baseline experiments.}
	\label{table:discussion:transformer:improvement_results}
\end{table}

The achieved accuracy scores by the models when fine-tuned with 90\% of data where in the high 99.x percent \ref{table:results:explainability:model_comparison} and even with only 1\% of data used for training, accuracy was still over 99\%. The flow representation used omits the packet payload, so payload based attacks like \textit{SQL Injection} or \textit{XSS} attacks are very hard, if not impossible, to detect. There might simply be very little room for improvement so even if pre-training where effective, it would not show up in our results. One approach to mitigate this effect would be to switch to multinomial classification of the exact attack type instead of binary classification. This approach increases fine-tuning training time, but since 
unsupervised pre-training takes disproportionately longer than fine-tuning at the moment, overall training time would not increase significantly. \par

In accordance with the last point, the datasets we used seem to be easy to classify. Even though not completely comparable, the results from the \gls{dtc} alone show that records in both datasets can be classified with high accuracy, i.e. about 97\% or more \ref{table:results:explainability:model_comparison} even without access to flow information.
The lack of complexity of the data might be another reason why pre-training in this case showed no improvement as supervised training was always sufficient to extract all relevant information from the input data. To inspect this hypotheses, a more challenging dataset should be used or a combination of different datasets. Ideal would be actual captured traffic of a mid size company during a penetration test. \par

Word embeddings are an integral part of model \gls{nlp} systems. This aspect is not included in our process since our feature vectors where already given. As our feature representation contains some qualitative features which are mapped to a mostly arbitrary number e.g. the IP protocol identifier, creating embeddings for qualitative features and tuning them to the task at hand might lead to improvement in overall accuracy, independent of whether pre-training is used or not. \par
	
Insufficient data or model complexity might be another reason why pre-training has produced no consistent improvement. Google BERT \cite{bert} uses a model with 110 million parameters in its \textit{BASE} version and 340 million in for its \textit{LARGE} version. In comparison, our \gls{lstm} model consists of 5.3 million parameters and the transformer encoder model only contains 74 thousand parameters. In the original paper about BERT, google claims to have pre-trained their model on a corpus of about 3.3 billion words. During our work on this thesis many new \gls{nlp} models like Googles T5 \cite{google_t5}, Nvidias Megatron \cite{megatron} with the largest one being OpenAIs GPT-3 with 175 billion parameters, pre-trained on 410 billion tokens \cite{gpt3}. The datasets we are using contain around 25 million packets each which is two magnitudes smaller than the corpus Googles BERT was trained on. Larger models or significantly larger datasets would not have been feasible in our case due to long training times. One of these two reasons, i.e. either lack of model complexity or pre-training data, might be the cause of our inconclusive results. This hypotheses is of course not easily checked without the necessary resources. As already stated above, the dataset used for unsupervised pre-training could be magnified by merging multiple datasets together or even using available unlabeled network traffic data for pre-training. \par

Unsuited proxy tasks could also be a reason why pre-training showed little effect. The used proxy tasks appeared to us as most intuitive but other choices might have been more effective. There is also the possibility of pre-training using labeled where e.g. a custom dataset is constructed where flows are labeled with the application layer protocol the flow is part of. Other unsupervised training methods might also be feasible e.g. energy-based unsupervised learning \cite{energy_based_training}. The possible approaches here are many and we have by no means explored all our options. \par

Looking at the validation and training loss progression during supervised fine-tuning, it shows that especially in experiments where models are trained on only 1\% of the dataset or even less, i.e. with the specialized subsets, the models start to overfit heavily. It might be that the effects of over-fitting in these scenarios mitigates any positive effect pre-training might have had. During fine-tuning the model learns two things: Patterns in the data based on the label, but also how to classify data at all. Classification is mostly learned by the fully connected layer at the output stage, but not only. During pre-training the model might have constructed a perfectly useful latent feature space, but it has not yet learned how to do actual classification based on it. This poses the difficulty of teaching the model how to classify, but not overfit it on the little data it has, overwriting any possible gains from pre-training. Deducted from this hypothesis it should be a requirement that the time the model takes to converge should be greater than the time the model learns to classify at all. This problem is implicitly mitigated by increasing the size of the dataset.\par

Furthermore, pre-training was performed with parts of the same dataset which was also used for supervised fine-tuning and for validation. Even though the labels where ignored, the data used for pre-training contained attack flows. If anything, this is most likely beneficial for possible positive effects on the final accuracy and metrics and has to be considered when trying to reproduce the results. This effect could be mitigated by e.g. using one dataset for pre-training and a different dataset for fine-tuning. In a real world setting however our scenario is more likely to apply as for pre-training an \gls{nids} it would make most sense to train the model with network traffic captured within the network the system is going to protect. This would also mean that it has to be assumed that the data also contains some attack flows as there is no way of asserting otherwise. For a real world application this approach also assures that the models learn common patterns of the specific networks they are going to be used in. It might even make sense to re-train or at least update the model periodically with recent data. Unfortunately network protocols are not as universal as natural language which makes transfer learning more difficult .i.e. it would be hard to teach the model universal patterns of network traffic. An ad-hoc approach tailored to the traffic of a single specific network is more likely to yield usable results in the near future. \par

An unsuited data representation might stifle the effectiveness of the used models. Even after deciding to use a flows representation, there is a wide selection of feature spaces \cite{feature_vectors}.
As our analysis of the datasets in section \ref{sec:results:explainability:dtc} i.e. in particular table \ref{table:results:expl:fimp_90_ds} shows that a lot of importance is accumulated in a few features while others are mostly irrelevant which shows that the selection of the correct feature and data representation has a significant effect on the 
performance of the models. We used a flow representation of per-packet feature vectors instead of the often used approach of aggregating all packets of a flow into a single feature vector containing statistical data. This was, as already explained in earlier sections, to enable the use of machine learning techniques used in \gls{nlp} which almost exclusively expect a sequences of tokens as input. When considering the immense overhead of using sequences instead of a single vector and looking at the results of the \gls{dtc} which has no concept of flows but still performs reasonably well, there might be better ways to build meaningful sequences. An alternative approach might be to    \cite{kitsune}.




Discuss any open issues and give a critical reflection of your work. E.g., what could be problems to deploy your method or do you have an idea how your findings could be generalized or what could be a hindrance for generalization?

Also discuss strange things you observed or results you could not completely explain. 

\newpage