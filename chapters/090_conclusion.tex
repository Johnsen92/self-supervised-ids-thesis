\chapter{Conclusion} \label{sec:conclusion}

The goal of our research was to explore possible applications of advancements made in other domains like \gls{nlp} or visual computing to the field of \gls{nids}. Possible combinations of different data representations, models, parameterization and techniques span a vast design space which we carefully navigated to cover as much ground as was possible within the time we were given. For this we selected two promising models for sequence processing: A \gls{rnn} with \gls{lstm} cells and the attention based transformer model to perform binary classification on the \gls{ids} datasets CIC-IDS-2017 and UNSW-NB15. For pre-training, we devised different tasks for the models which would force them to find patterns and structure in the data and which could be evaluated without the need for human made labels. With the powerful PyTorch suite at its core, we developed a framework in about 5000 lines of Python code to automate training and results generation to make them as reproducible as possible. With an array of 66 experiments we tried to unearth any potential improvements pre-training might yield. As even this was not enough to give us a definitive direction to pursue, we dug deeper into the inner workings of the models and the structure of the data to maybe shed light on what worked and what did not. The result of our endeavor is a broad overview of possible approaches to self-supervised training on state-of-the-art machine learning models and an in-depth look into the patterns and structures in the data which allow the models to learn and improve during training. Although results where mostly inconclusive, it is to consider that our experiments were far from exhaustive. As we discussed in previous sections, the datasets might simply be too easy to classify and there might have been little room for results to be improved by pre-training when compared to purely supervised training. One might try pre-training with more data, or with a different dataset than is used for supervised training. Different, cleverer proxy tasks might be a way to make pre-training effective. Just trying out different kinds of auto-encoders, of which there are many by now, might yield interesting results. When it comes to the broad topic of self-supervised learning, there is also the possibility of energy based self-supervised learning which does not require any labeled data at all to work for binary classification.
We contributed to the topic at hand by delivering a promising primer and ideas which might act as a starting point for further inquiries. 

Conclude your work. Stress again what was the contribution. 
Provide an outlook what could be further improvements and what could future research do to continue your work.

\newpage