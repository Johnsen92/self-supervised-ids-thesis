\chapter{Conclusion} \label{sec:conclusion}

The goal of our research was to explore possible applications of advancements made in other domains like \gls{nlp} or visual computing to the field of \gls{nids}. In particular we looked at pre-training with proxy tasks that has been successfully applied to solve \gls{nlp} problems. Possible combinations of different data representations, models, parameterization and techniques span a vast design space which we carefully navigated to cover as much ground as was possible with the resources, computational and temporal, at hand. For this we selected two promising models for sequence processing: A \gls{rnn} with \gls{lstm} cells and the attention based transformer model to perform binary classification on the \gls{nids} datasets CIC-IDS2017 and UNSW-NB15. For pre-training, we devised different tasks for the models which would force them to find patterns and structure in the data and which could be evaluated without the need for human made labels. With the powerful PyTorch suite at its core, we developed a framework in about 5000 lines of Python code to automate training and results generation to make them as reproducible as possible. With an array of 66 experiments we tried to unearth any potential improvements pre-training might yield. As even this was not enough to give us a definitive direction to pursue, we dug deeper into the inner workings of the models and the structure of the data to maybe shed light on what worked and what did not and why. The result of our endeavor is a broad overview of possible approaches to self-supervised training on state-of-the-art machine learning models and an in-depth look into the patterns and structures in the data which allow the models to learn and improve during training. Although results where mostly inconclusive or insufficient for generalization, it is to consider that our experiments were far from exhaustive. As we discussed in previous sections, the datasets might simply be too easy to classify and there might have been little room for results to be improved by pre-training when compared to purely supervised training. One might try pre-training with more data, or with a different dataset than is used for supervised training. Different, cleverer proxy tasks might be a way to make pre-training effective. Just trying out different kinds of auto encoders, of which there are many by now, might yield interesting results. When it comes to the broad topic of self-supervised learning, there is also the possibility of energy based self-supervised learning which does not require any labeled data at all to work for binary classification. We contributed to the topic at hand by delivering a promising primer and ideas which might act as a venture point for further inquiries. The code we produced and the lessons we learned during the way are documented and will hopefully guide future approaches of self-supervised machine learning based \gls{nid}.

\newpage