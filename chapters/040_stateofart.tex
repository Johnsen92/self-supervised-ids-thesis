\chapter{State of the art} \label{sec:stateofart}

As the topic of this thesis is rather specific, comparable research is hard to find. Overall, the thesis works on the two subjects of unsupervised pre-training for \glspl{nn} and for \gls{ml} supported \gls{nids}. Here we are looking at state-of-the-art research of both aspects individually.

\section{Self-supervised Pre-training for LSTMs and Transformer Networks}

When it comes to machine learning, rapid progress has been made over the past years. Frameworks such as PyTorch \cite{pytorch} and Tensorflow \cite{tensorflow} have made the technology accessible to people without a background in computer science. More than 11 thousand papers in the category "Computer Science - Artificial Intelligence (cs.AI)" have been published on arXiv.org \cite{arxiv} within only the last year. With steadily increasing processing capabilities, vast amounts of data can be used to train ever growing \glspl{nn} within an acceptable timeframe. E.g. the largest variant of Google's \gls{bert} algorithm has 340 million parameters and was trained on a dataset of 3.3 Billion words \cite{bert}. \par

\subsection{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}

Google's BERT \cite{bert} by Jacob Devlin et al. effectively uses a Deep Bidirectional Transformer model, often referred to as Transformer Encoder, for various \gls{nlp} tasks, both on sentence and word level, like question answering, natural language inference, sentiment analysis, paraphrasing and others. At the time it was published, it produced the highest recorded GLUE \cite{glue} score of 80.5\% advancing it by 7.7\% over the former top scorer. It uses the WordPiece \cite{wordpiece} embedding resulting in a 30,000 token vocabulary. It was pre-trained in a fully unsupervised fashion on all sentences in the English Wikipedia (2,5 Billion words) and the BooksCorpus \cite{books_corpus} containing 800 Million word. The pre-training consisted of two proxy tasks: \gls{nsp} and \gls{mlm}. For \gls{nsp}, two sections of text, A and B, separated by a [SEP] token are fed into the model at the same time. 50\% of the time, B is the next section that follows A in the original text. 50\% of the time it is a random sentence from the corpus. The model is tasked with predicting, if sentence B follows sentence A. For \gls{mlm}, 15\% of the input tokens are hidden from the model by replacing with a [MASK] tokens. The model is tasked with reconstructing the masked tokens. Both of those pre-training tasks are performed at the same time. The pre-trained model is then fine-tuned to perform a specific down-stream task. \par
This two stage approach, pre-training and fine-tuning, produces a reusable pre-trained model which can then be fine-tuned relatively swiftly (Jacob Devlin et al. state that it takes at most an hour of fine-tuning on a \gls{gpu} to replicate all results in the paper) to solve various \gls{nlp} tasks. For this thesis, we use the same approach to pre-train our models in an unsupervised fashion and then fine-tune them with a small amount of labeled data to teach them the down-stream task of classifying network flows. We also use the pre-training task of masking parts of the input data for the model to reconstruct for both our \gls{lstm} and Transformer networks. The \gls{nsp} task is not feasible in our situation, as network flows don't have an order other than the time of occurrence, and therefore flows don't have a semantically identifiable successor or predecessor.

\subsection{Unsupervised Learning of Video Representations using LSTMs} \label{sec:stateofart:unsupervised_video_lstm}

The use of unsupervised learning is not limited to Transformer networks. As early as 2016, before the rise of Transformers, Nitish Srivastava et al. showed in their paper "Unsupervised Learning of Video Representations using LSTMs" \cite{unsupervised_learning_lstms} that unsupervised learning on \glspl{lstm} can have a positive impact on subsequent classification tasks. The authors use video data to train their models in frame prediction and auto encoding as the proxy tasks with the goal of improving accuracy in human action recognition, based on evaluation with the UCF-101 and
HMDB-51 datasets. They experimented with two types of video representations: patches of image pixels and high-level representations ("percepts") of video frames extracted by a convolutional net. They used 13,320 videos with an average length of 6.2 seconds belonging to 101 different action categories. \par
The auto-encoding property of the model is achieved by concatenating two \glspl{lstm}, with one performing the function of encoder and one of decoder. The goal is to produce a sequence2sequence model capable of reconstructing the input sequence after being forced to compress the input data. The input sequence is first processed by the encoder \gls{lstm} to produce an output of constant length (in their case, the hidden size of the encoder \gls{lstm}). The resulting vector is then fed into the decoder which is tasked with reconstructing the input sequence in reverse order. Here, the decoder can be configured to either be \textit{conditioned} or \textit{unconditioned}. A conditioned decoder uses the output of the last \gls{lstm} stage as input for the next stage. An unconditioned decoder uses the corresponding input token (ground truth) as input for the next stage. The latter practice is also called \textit{teacher forcing}. \par
The second unsupervised task to train the \gls{lstm} consists of predicting multiple future video frames. For this, again two consecutive \gls{lstm} networks are used: an encoder and a predictor \gls{lstm}. The first network is fed the frame representation of part of a short video and again produces a fixed sized output vector to be used by the predictor \gls{lstm}. The second \gls{lstm} is then tasked with producing the remaining frames. Same as with the auto-encoder the predictor \gls{lstm} can either be conditioned or unconditioned. \par
The authors then proposed a composite model as can be seen in figure \ref{fig:stateofart:unsupervised_lstm_composite} where both proxy tasks, reconstructing the input and predicting the future, are combined to produce a single model.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{img/unsupervised_learning_with_lstms_composite.png}
	\caption{Composite model for input reconstruction and future prediction \cite{unsupervised_learning_lstms}}
	\label{fig:stateofart:unsupervised_lstm_composite}
\end{figure}

The pre-trained models are then finetuned for their classification task on the mentioned training datasets.
With the pre-trained + finetuned composite model, the authors achieved an absolute increase of 1.3\% accuracy for both the UCF-101 and the HMDB-51 datasets over a conventional \gls{lstm} classifier as can be seen in table \ref{table:stateofart:unsupervised_learning_results}.

\begin{table}[]
	\begin{tabular}{l c c c}
		\thead{Model} & \thead{UCF-101 \\ RGB} & \thead{UCF-101
\\ 1-frame flow} & \thead{HMDB-51 \\
RGB} \\ \hline
		\midrule
		Single Frame & 72.2 & 72.2 & 40.1 \\
		\midrule
		LSTM classifier & 74.5 & 74.3 & 42.8 \\
		\midrule
		Composite LSTM
Model + Finetuning & 75.8 & 74.9 & 44.1 \\
	\end{tabular}
	\caption{Summary of results on Action Recognition \cite{unsupervised_learning_lstms}}
	\label{table:stateofart:unsupervised_learning_results}
\end{table}

For our thesis we used the same Auto-Encoder and composite model for pre-training as explained in sections \ref{sec:experiments:lstm_auto_encoder} and \ref{sec:experiments:lstm_composite}. We tried with both conditioned and unconditioned models, comparing results in section \ref{sec:results}.

\subsection{Unsupervised pre-training ofa Deep LSTM-based StackedAutoencoder for Multivariate timeSeries forecasting problems} \label{sec:stateofart:unsupervised_learning_lstms_timeseries}

In their 2019 paper \cite{unsupervised_learning_lstms_timeseries} Alaa Sagheer et al. explore the benefits of unsupervised pre-training using stacked \gls{lstm} auto encoders with subsequent supervised finetuning. Their goal was to improve the prediction capabilities for \gls{mts} problems. In their previous paper \cite{dlstm_time_series_forecasting}, the authors showed the effectiveness of \gls{dlstm} based models for \gls{mts} prediction tasks. In their 2019 paper, they showed the improvements resulting from pre-training when compared to an initial random initialization of weights when working with \gls{dlstm} models. Compared to shallow \gls{lstm} networks, \gls{dlstm} networks contain multiple layers of \gls{lstm} cells stacked on each other. Information travels the network from left to right and from bottom to top as is depicted in figure \ref{fig:stateofart:unsupervised_learning_with_lstms_dlstm}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{img/unsupervised_learning_with_lstms_dlstm.png}
	\caption{Depiction of a \gls{dlstm} with two layers \cite{unsupervised_learning_lstms}}
	\label{fig:stateofart:unsupervised_learning_with_lstms_dlstm}
\end{figure}

For pre-training the network, the authors use a \gls{lstmsae} model. In contrast to a conventional auto encoder like described in \ref{sec:backgrund:autoencoder}, a stacked auto encoder model uses multiple encoder and decoder layers as can be seen in figure \ref{fig:stateofart:unsupervised_learning_dlstm_mts_lstmsae}. The different encoder layers are trained individually in a multi phased training procedure: Train the first auto encoder layer like a conventional \gls{lstmae} with the target being the original input data. Cut the decoder part of the first \gls{lstmae}. When training the second \gls{lstmae}, the input is encoded by both the encoder of the first and second \gls{lstmae} block and then decoded only by the decoder of the second \gls{lstmae}. The target data for training the second \gls{lstmae} is again the original input, and not the reconstructed input of the first \gls{lstmae}. The training process is depicted in figure \ref{fig:stateofart:unsupervised_learning_dlstm_mts_lstmsae}. This process is then repeated for arbitrarily many \gls{lstmae} layers. The authors tried both one and two stacked layers of \gls{lstmae}. The trained encoder blocks are then used to initialize a multi layered \gls{dlstm}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{img/unsupervised_learning_dlstm_mts_lstmsae.png}
	\caption{Layer-wise pre-training of \gls{lstmsae} model. \cite{unsupervised_learning_lstms_timeseries}}
	\label{fig:stateofart:unsupervised_learning_dlstm_mts_lstmsae}
\end{figure}

To complete the training phase, the parameters of the pre-trained \gls{dlstm} are then finetuned in a supervised fashion. This is done by adding an output layer which produces values of the dimension of labels of the training set used and of the variables to be predicted. In the case of the authors, the output layer was a single neuron to predict a single variable. For supervised finetuning and validation they used two datasets:

\begin{enumerate}
	\item \textit{The capital bike sharing dataset}
	\item \textit{The PM2.5 concentration in the air of CHINA dataset}
\end{enumerate}

For the first dataset, the model tried to predict how many bike will be rented on a particular day based on parameters like \textit{Season, Holiday, Weekday, Working day}, etc. For the second dataset, the task was to predict PM2.5 concentrations in the air for various Chinese cities based on parameters like \textit{Dew Point, Temperature, Pressure, Combined Wind Direction,} etc. \par

As metric of performance, the authors used \gls{rmse}, \gls{mae} and \gls{smape} (lower is better) which all describe the difference between predicted value and observed value. The results of evaluation with both data sets can be seen in tables \ref{table:stateofart:unsupervised_learning_lstms_timeseries_results1} and \ref{table:stateofart:unsupervised_learning_lstms_timeseries_results2}.

\begin{table}[]
	\begin{tabular}{l c c c c c c c}
		\thead{Model} & \thead{No. of \\ hiddenlayer} & \thead{Dropout} & \thead{lag} & \thead{batch} & \thead{RMSE} & \thead{MAE} & \thead{SMAPE} \\ \hline
		\midrule
		DLSTM & 1 & 0.4 & 20 & 146 & 52.062 & 32.468 & 12.088 \\
		DLSTM & 2 & 0.3 & 25 & 219 & 49.811 & 31.524 & 12.183 \\
		LSTM-SAE & 1 & 0.1 & 30 & 219 & 49.389 & 32.192 & 13.878 \\
		LSTM-SAE & 2 & 0.1 & 30 & 73 & 46.927 & 30.041 & 11.646 \\
	\end{tabular}
	\caption{The results of \gls{dlstm} and \gls{lstmsae} using data set 1 \cite{unsupervised_learning_lstms_timeseries}}
	\label{table:stateofart:unsupervised_learning_lstms_timeseries_results1}
\end{table}

\begin{table}[]
	\begin{tabular}{l c c c c c c c}
		\thead{Model} & \thead{No. of \\ hiddenlayer} & \thead{Dropout} & \thead{lag} & \thead{batch} & \thead{RMSE} & \thead{MAE} & \thead{SMAPE} \\ \hline
		\midrule
		DLSTM & 1 & 0.2 & 30 & 60 & 23.993 & 12.124 & 10.919 \\
		DLSTM & 2 & 0.2 & 30 & 73 & 23.750 & 12.452 & 12.181 \\
		LSTM-SAE & 1 & 0.1 & 30 & 219 & 23.907 & 12.509 & 11.052 \\
		LSTM-SAE & 2 & 0.3 & 25 & 146 & 24.041 & 12.060 & 9.864 \\
	\end{tabular}
	\caption{The results of \gls{dlstm} and \gls{lstmsae} using data set 2 \cite{unsupervised_learning_lstms_timeseries}}
	\label{table:stateofart:unsupervised_learning_lstms_timeseries_results2}
\end{table}

The results in tables \ref{table:stateofart:unsupervised_learning_lstms_timeseries_results1} and \ref{table:stateofart:unsupervised_learning_lstms_timeseries_results2} show that unsupervised pre-training improved final accuracy and led to better and faster convergence. \par

For this thesis, we used only a single \gls{lstmae} network, but with three layers of \gls{lstm} cells making it a \gls{dlstm} for both encoder and decoder.

\section{Machine Learning for Network Intrusion Detection}


Here provide an overview of the related state of art. Look for papers that are closest to the research you are doing
Suggestion: make a table with the related papers and compare them wrt to different criteria, for instance

\begin{itemize}
	\item Findings: What do they claim (main findings)
	\item Data: What data set they are using
	\item Methods: Which methods did they use?
	\item Reproducibility: Is it possible to reproduce the results? (e.g., is the data available? are all parameter settings provided? Is source code provided?)
	\item Relevance (How relevant is it for your work)
\end{itemize}


In the last paragraph explain how your work differs from the existing works.



\newpage
