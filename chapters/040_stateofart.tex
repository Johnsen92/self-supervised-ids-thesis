\chapter{State of the art} \label{sec:stateofart}

As the topic of this thesis is rather specific and novel in the context if \gls{nid}, comparable research is hard to find. Overall, the thesis works on the two subjects of self-supervised pre-training for \glspl{nn} and for \gls{dl} supported \gls{nids}. Here we are looking at state-of-the-art research of both aspects individually. Special focus lays on achievements in machine learning based \gls{nlp} due to the similar structure in input data and semantic.

\section{Machine Learning for Network Intrusion Detection} \label{sec:stateofart:nid}

Machine learning techniques have shown to be competitive to signature based expert systems when it comes to \gls{nid}. Hence there have been many attempts over the past years to implement various types of machine learning algorithms with considerable success. The design space for such systems is vast, as Hongyu Liu and Bo Lang show in their 2019 paper \cite{nid_ml_survey_2019} "Machine Learning and Deep Learning Methods for Intrusion Detection Systems: A Survey" where they enumerate the possible approaches to a machine learning based \gls{ids} and discuss their advantages and disadvantages. The authors classify proposed \glspl{ids} to date based on data sources and detection methods used to create a comprehensive taxonomy system. When using machine learning, further classification entails the type of machine learning methods used for implementing it. Based on their classification, our approach would be categorized as follows: the data source is a network based IDS with flow based detection using deep learning methods as depicted in figure \ref{fig:stateofart:ids_data_source_taxonomy}. As can be see in figure \ref{fig:stateofart:ids_detection_methods_taxonomy}, the detection method is machine learning based anomaly detection and the machine learning model is a deep learning model based on \gls{lstm} and transformer networks, but using both unsupervised and supervised learning methods. As such, our model is not even completely classifiable by the taxonomy proposed by Hongyu Liu et al.. \par

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{img/ids_data_source_taxonomy.png}
	\caption{Our design decisions (blue) and alternatives (green) based on the data source taxonomy proposed by Hongyu Liu et al. \cite{nid_ml_survey_2019}.}
	\label{fig:stateofart:ids_data_source_taxonomy}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{img/ids_detection_methods_taxonomy.png}
	\caption{Our design decisions (blue) and alternatives (green) based on the detection method taxonomy proposed by Hongyu Liu et al. \cite{nid_ml_survey_2019}.}
	\label{fig:stateofart:ids_detection_methods_taxonomy}
\end{figure}

Congruent with our assumptions, Hongyu Liu and Bo Lang deem a flow based detection to be a suitable way to structuring the raw network data, as it represents the whole network environment and retains a lot of contextual information, but with the obvious drawback that package payload is being ignored. This leads to poor detection rates for \gls{u2r} and \gls{r2l} attacks like SQL injection or XSS attacks, which our results confirm as can be seen in section \ref{sec:results}. \par

Although the design space for \gls{ml} based approaches is much greater, we are focusing on \gls{dl} based approaches, which have shown to be superior to shallow networks in general. A good initial overview of state-of-the-art \gls{dl} techniques applied to \gls{nid} can be found in the 2020 paper "Fog-Based Attack Detection Framework for Internet of Things Using Deep Learning" \cite{fog_based_detection_survey_2020} by Ahmed Samy et al. Although the paper focuses on \glspl{ids} for resource and energy constrained \gls{iot} networks, the authors provides a comprehensive overview of the performance of different up-to-date \gls{dl} models applied to various \gls{nids} data sets. The goal of their research is to implement \gls{dl} based \glspl{ids} in a resource constrained \gls{iot} network by outsourcing the processing to fog nodes, which are capable of storing large amounts of data with low latency and are placed at the edge of the \gls{iot} network. In the context of their research, they implemented six state of the art \gls{dl} models:

\glsreset{dnn}
\glsreset{lstm}
\glsreset{bilstm}
\glsreset{cnnlstm}
\glsreset{gru}
\glsreset{cnn}

\begin{itemize}
	\item \textbf{\gls{dnn}} with an input layer of 1024 cells, five hidden layers with 512 cells each using \gls{relu} activation functions and one output layer with one cell using a sigmoid activation function.
	\item \textbf{\gls{lstm}} with an input layer of 128 cells, three hidden layers with 256 cells each and one output layer with one cell using sigmoid activation 
	\item \textbf{\gls{bilstm}} with an input layer of 128 cells, three hidden layers with 128 cells each and one output layer with one cell using sigmoid activation
	\item \textbf{\gls{cnnlstm}} with three convolutional layers with 64 filters, each using \gls{relu} activation functions, three pooling layers, one \gls{lstm} layer with 256 cells and one output layer with one cell using sigmoid activation
	\item \textbf{\gls{gru}} with an input layer of 64 cells, three hidden layers with 64 cells each and one output layer with one cell using sigmoid activation
	\item \textbf{\gls{cnn}} with three convolutional layers with 64 filters each using \gls{relu} activation functions, three pooling layers and one output layer with one cell using sigmoid activation
\end{itemize}

For multi-class classification the output layer is expanded to match the number of classes in the data set. Ahmed Samy et al. tested their implementations on five different data sets:

\begin{itemize}
	\item UNSW-NB15 \cite{unsw_nb15}
	\item CICIDS-2017 \cite{cic_ids_2017}
	\item RPL-NIDS17 \cite{rpl_nids17}
	\item N\_BaIoT \cite{n_baiot}
	\item NSL-KDD \cite{nsl_kdd}
\end{itemize}

They used a feature representation of the data comprised of 80 network traffic features extracted with CICFLOWMETER. The best results in training were achieved with a learning rate of 0.01, a batch size of 64 using the Adam optimization algorithm. They used accuracy, precision, recall, F1-measure, \gls{far}, \gls{dr} as metrics corresponding with the definitions in section \ref{sec:background:metrics} to assess the performance of the different models. This research was especially relevant for u, because the authors used similar models and the same datasets, so a direct comparison with our results can be made. An overview of the relevant results for binary classifications can be found in table \ref{table:stateofart:fog_based_detection_survey_2020_results_binary} and for multi-class classification in table \ref{table:stateofart:fog_based_detection_survey_2020_results_class}.

\begin{table}[]
	\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c|c|c|c|c}
		\textbf{DataSet Name}                         & \textbf{DL Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Measure} & \textbf{FAR} & \textbf{DR} \\ \midrule
		\multirow{6}{*}{\makecell{\textbf{UNSW-NB15} \\ \textbf{Dataset} }}   & \textbf{DNN}      & 99.67\%         & 99.79\%          & 99.87\%       & 99.83\%           & 6.23\%     & 99.87\%   \\
		& \textbf{LSTM}     & 99.96\%         & 99.96\%          & 99.97\%       & 99.98\%           & 4.02\%     & 99.97\%   \\
		& \textbf{Bi-LSTM}  & 99.67\%         & 99.82\%          & 99.83\%       & 99.83\%           & 5.35\%     & 99.83\%   \\
		& \textbf{GRU}      & 99.58\%         & 99.79\%          & 99.77\%       & 99.78\%           & 6.21\%     & 99.77\%   \\
		& \textbf{CNN}      & 99.66\%         & 99.86\%          & 99.78\%       & 99.82\%           & 4.24\%     & 99.78\%   \\
		& \textbf{CNN-LSTM} & 98.95\%         & 99.96\%          & 98.97\%       & 99.46\%           & 1.2\%      & 98.97\%   \\ \midrule
		\multirow{6}{*}{\makecell{\textbf{CIC-IDS2017} \\ \textbf{Dataset}}} & \textbf{DNN}      & 98.95\%         & 98.57\%          & 99.73\%       & 99.15\%           & 2.29\%     & 99.73\%   \\
		& \textbf{LSTM}     & 99.37\%         & 99.28\%          & 99.67\%       & 99.49\%           & 1.15\%     & 99.67\%   \\
		& \textbf{Bi-LSTM}  & 99.35\%         & 99.22\%          & 99.77\%       & 99.48\%           & 1.25\%     & 99.77\%   \\
		& \textbf{GRU}      & 99.35\%         & 99.21\%          & 99.73\%       & 99.47\%           & 1.26\%     & 99.73\%   \\
		& \textbf{CNN}      & 99.08\%         & 98.61\%          & 99.92\%       & 99.26\%           & 2.25\%     & 99.92\%   \\
		& \textbf{CNN-LSTM} & 98.88\%         & 98.41\%          & 99.8\%        & 99.1\%            & 2.57\%     & 99.8\%   
	\end{tabular}}
	\caption{Evaluation metrics of \gls{dl} models with the five data sets in binary classification. \cite{fog_based_detection_survey_2020}}
	\label{table:stateofart:fog_based_detection_survey_2020_results_binary}
\end{table}

\begin{table}[h]
	\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c|c|c|c|c|c|c}
		& \multicolumn{3}{c}{\textbf{DNN}}     & \multicolumn{3}{c}{\textbf{LSTM}}    & \multicolumn{3}{c}{\textbf{Bi-LSTM}}  \\ \midrule
		\textbf{Attacks} & \textbf{P} & \textbf{R} & \textbf{F1-M} & \textbf{P} & \textbf{R} & \textbf{F1-M} & \textbf{P}  & \textbf{R} & \textbf{F1-M} \\ \midrule
		\textbf{Benign} & 98.41\%  & 99.71\%  & 99.06\%     & 99.48\%  & 99.15\%  & 99.31\%     & 99.04\%   &  99.24\%  & 99.14\%     \\
		\textbf{DDoS} & 99.18\%  & 57.77\%  & 73.01\%     & 98.89\%  & 98.94\%  & 98.91\%     & 99.49\%   &  97.16\%  & 98.31\%     \\
		\textbf{FTP-Patator} & 84.38\%  & 56.41\%  & 67.62\%     & 91.05\%  & 99.14\%  & 94.93\%     & 93.84\%   &  98.94\%  & 96.32\%     \\
		\textbf{Port-Scan} & 47.66\%  & 74.78\%  & 58.21\%     & 99.42\%  & 99.91\%  & 99.66\%     & 99.21\%   &  99.97\%  & 99.59\%     \\
		\textbf{SSH-Patator} & 39.31\%  & 29.27\%  & 33.56\%     & 100\%    & 70.68\%  & 82.82\%     & 93.73\%   &  79.03\%  & 85.75\%     \\
		\textbf{Brute-Force} & 30.23\%  & 4.12\%   & 7.26\%      & 44.74\%  & 84.65\%  & 58.54\%     & 44.81\%   & 85.77\%  & 58.86\%     \\
		\textbf{SQL-Injection} & 0\%      & 0\%      & 0\%         & 11.11\%  & 2.56\%   & 4.28\%      & 0\%       0\%      & 0\%         \\ \midrule
		& \multicolumn{3}{c}{\textbf{GRU}}     & \multicolumn{3}{c}{\textbf{CNN}}     & \multicolumn{3}{c}{\textbf{CNN-LSTM}} \\ \midrule
		\textbf{Attacks} & \textbf{P} & \textbf{R} & \textbf{F1-M} & \textbf{P} & \textbf{R} & \textbf{F1-M} & \textbf{P} & \textbf{R} & \textbf{F1-M} \\ \midrule
		\textbf{Benign} & 99.81\%  & 98.86\%  & 99.33\%     & 95.78\%  & 99.94\%  & 97.82\%     & 74.5\%    & 99.89\%  & 85.35\%     \\
		\textbf{DDoS} & 99.06\%  & 98.94\%  & 99        & 99.96\%  & 98.68\%  & 99.32\%     & 99.77\%   & 98.93\%  & 99.35\%     \\
		\textbf{FTP-Patator} & 89.13\%  & 99.79\%  & 94.16\%     & 99.76\%  & 84.58\%  & 91.54\%     & 96.42\%   & 49.2\%   & 65.16\%     \\
		\textbf{Port-Scan} & 99.79\%  & 99.93\%  & 99.86\%     & 99.96\%  & 91.28\%  & 85.42\%     & 91\%      & 0.57\%   & 1.13\%      \\
		\textbf{SSH-Patator} & 74.04\%  & 98.76\%  & 84.63\%     & 98.57\%  & 50.64\%  & 66.91\%     & 99.56\%   & 50.98\%  & 67.43\%     \\
		\textbf{Brute-Force} & 43.8\%   & 84.65\%  & 57.73\%     & 26.14\%  & 3.16\%   & 5.64\%      & 0\%       & 0\%      & 0\%         \\
		\textbf{SQL-Injection} & 0\%      & 0\%      & 0\%         & 0\%      & 0\%      & 0\%         & 0\%       & 0\%      & 0\%        
	\end{tabular}}
	\caption{Evaluation metrics of \gls{dl} models with the five data sets in binary classification. P denotes precision, R recall and F1-M the F1 measure. \cite{fog_based_detection_survey_2020}}
	\label{table:stateofart:fog_based_detection_survey_2020_results_class}
\end{table}

As shown by the results and also stated in the paper, \gls{lstm} networks outperform other deep learning models consistently in attack detection and accuracy overall. This gives us confidence that our decision of working with \gls{lstm} networks was the right call as it adds to the real world relevance of our work. Additionally, we use a transformer encoder model for classification which was not included in the comparison done by Ahmed Samy et al. Furthermore, the models in their paper were trained exclusively in a supervised manner with at least 70\% of the data available in each dataset which differs from our approach of using very little labeled data and relying on self-supervised pre-training. \par

Compared to other machine learning models there is much less research published on transformer or attention based networks for \gls{ids}. In one of the few papers published on the topic by Mangxuan Tan et al., the authors constructed their own attention based model, similar to the classic transformer model \cite{attention_model_ids}, which they coined "Attention for Network Intrusion Detection (ANID)". The model includes a \gls{ff}, a \gls{sda} and a \gls{sdsa} component which are the same as in the traditional transformer model by Vaswani et al. but combined differently. Like us, the authors used the CIC-IDS2017 dataset for their experiments but use a fundamentally different approach to data pre-processing and feature selection. For pre-processing, the authors constructed a sequenced of feature vectors by dividing the raw pcap traces into time slots of 0.5 seconds and labeling them as 1 (attack class) if an attack occurred in that time frame and 0 (benign class) otherwise. In the resulting pre-processed dataset, each record contains a sequence of 10 sequential time slots. As feature representation, they use a selection of statistical features aggregated over each time slot resulting in 19 features \cite{attention_model_ids}. Furthermore, they removed 95\% of attack slots of each attack type to simulate a real world scenario where attacks are much rarer than they are in the dataset. They compared the performance of their model to a conventional \gls{bilstm} model and a \gls{clf}. Their results can be seen in table \ref{table:stateofart:attention_model_ids_results}.

\begin{table}[]
	\centering
	\begin{tabular}{l c c c c}
		\thead{Model} & \thead{Precision (\%)} & \thead{Recall (\%)} & \thead{F-Score (\%)} & \thead{FPR (\%)} \\ \hline \midrule
		Bi-LSTM & 88.86 & 91.85 & 90.33 & 0.15 \\
		CRF 	& 48.09 & 15.41 & 23.34 & 0.22 \\
		ANID 	& 97.29 & 94.40 & 95.28 & 0.03 \\
	\end{tabular}
	\caption{Model performance comparison for CIC-IDS2017 dataset \cite{attention_model_ids}}
	\label{table:stateofart:attention_model_ids_results}
\end{table}

Even though the authors used the same dataset as we did for our research, comparability with our models is limited due to the different approach to dataset pre-processing and feature selection. The paper still shows that attention based models can achieve a performance improvement over \gls{lstm} models in certain cases which gives us reason to look further into them.

\section{Self-supervised Pre-training for LSTMs and Transformer Networks}

When it comes to machine learning, rapid progress has been made over the past years. Frameworks such as PyTorch \cite{pytorch} and Tensorflow \cite{tensorflow} have made the technology accessible to people without a background in computer science. More than 11 thousand papers in the category "Computer Science - Artificial Intelligence (cs.AI)" have been published on arXiv.org \cite{arxiv} within the last year. With steadily increasing processing capabilities, vast amounts of data can be used to train ever growing \glspl{nn} within an acceptable timeframe.  E.g. the largest variant of Google's \gls{bert} algorithm has 340 million parameters and was trained on a dataset of 3.3 billion words \cite{bert}. Today, the limiting factor is often the amount of labeled data available to train a network. Hence, the topic of unsupervised or self-supervised training is explored by researchers of different fields in hope of enhancing their models without the need for more labeled data. Special attention was given to papers in the \gls{nlp} sector as the methods we use are successfully deployed there. 

In the following section we are looking at some of the related papers on the topic of self-supervised  or unsupervised training. \par

%\begin{adjustbox}{angle=90}[h]
%%	\begin{table}[h]
%	\scalebox{0.77}{
%		\begin{tabular}{c c c c c c}
%			\textbf{Paper} & \textbf{Model} & \textbf{Pre-training Task(s)} & \textbf{Downstream Task(s)} & \textbf{Val. Dataset(s)} & \textbf{Val. Value} \\ \hline
%			\midrule
%			\cite{bert} \ref{sec:stateofart:bert} & Transformer Encoder & next sentence prediction, word masking & various NLP tasks & GLUE score & 80.5\% acc. \\
%			\midrule
%			\cite{unsupervised_learning_lstms} \ref{sec:stateofart:unsupervised_video_lstm} & \acrshort{dlstm} & auto-encoding, sequence prediction & human action recognition & UCF-101, HMDB-51 & 74.9\%, 44.1\% acc. \\
%			\midrule
%			\cite{unsupervised_learning_lstms} \ref{sec:stateofart:unsupervised_video_lstm} & \acrshort{dlstm} & stacked auto-encoding & \acrshort{mts} forecasting & \makecell{capital bike sharing \\ PM2.5 concentration in the air of CHINA}  & 11.646, 9.864 \acrshort{smape} \\
%		\end{tabular}
%		\label{table:stateofart:self_supervised_overview}
%	}
%%	\end{table}
%\end{adjustbox}


\subsection{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding} \label{sec:stateofart:bert}

Google's BERT \cite{bert} by Jacob Devlin et al. effectively uses a deep bidirectional transformer model, often referred to as transformer encoder, for various \gls{nlp} tasks, both on sentence and word level, like question answering, natural language inference, sentiment analysis, paraphrasing and others. At the time it was published, it produced the highest recorded GLUE \cite{glue} score of 80.5\% advancing it by 7.7\% over the former top scorer. It uses the WordPiece \cite{wordpiece} embedding, resulting in a 30,000 token vocabulary. It was pre-trained in a fully unsupervised fashion on all sentences in the English Wikipedia (2,5 billion words) and the BooksCorpus \cite{books_corpus} containing 800 million words. The pre-training consisted of two proxy tasks: \gls{nsp} and \gls{mlm}. For \gls{nsp}, two sections of text, A and B, separated by a [SEP] token are fed into the model at the same time. 50\% of the time, B is the next section that follows A in the original text. 50\% of the time it is a random sentence from the corpus. The model is tasked with predicting if sentence B follows sentence A. For \gls{mlm}, 15\% of the input tokens are hidden from the model by replacing with a [MASK] tokens. The model is tasked with reconstructing the masked tokens. Both of those pre-training tasks are performed at the same time. The pre-trained model is then fine-tuned to perform a specific down-stream task. \par
This two stage approach, pre-training and fine-tuning, produces a reusable pre-trained model which can then be fine-tuned relatively swiftly (Jacob Devlin et al. state that it takes at most an hour of fine-tuning on a \gls{gpu} to replicate all results in the paper) to solve various \gls{nlp} tasks. For this thesis, we use the same approach to pre-train our models in an unsupervised fashion and then fine-tune them with a small amount of labeled data to teach them the down-stream task of classifying network flows. We also use the pre-training task of masking parts of the input data for the model to reconstruct for both our \gls{lstm} and Transformer networks. The \gls{nsp} task is not feasible in our situation, as network flows don't have an order other than the time of occurrence, and therefore flows do not have a semantically identifiable successor or predecessor.

\subsection{Unsupervised Learning of Video Representations using LSTMs} \label{sec:stateofart:unsupervised_video_lstm}

The use of unsupervised learning is not limited to transformer networks. As early as 2016, before the rise of transformers, Nitish Srivastava et al. showed in their paper "Unsupervised Learning of Video Representations using LSTMs" \cite{unsupervised_learning_lstms} that unsupervised learning on \glspl{lstm} can have a positive impact on subsequent classification tasks. The authors use video data to train their models in frame prediction and auto encoding as the proxy tasks with the goal of improving accuracy in human action recognition, based on evaluation with the UCF-101 and
HMDB-51 datasets. They experimented with two types of video representations: patches of image pixels and high-level representations ("percepts") of video frames extracted by a convolutional net. They used 13,320 videos with an average length of 6.2 seconds belonging to 101 different action categories. \par
The auto-encoding property of the model is achieved by concatenating two \glspl{lstm}, with one performing the function of encoder and one of decoder. The goal is to produce a sequence-to-sequence model capable of reconstructing the input sequence after being forced to compress the input data. The input sequence is first processed by the encoder \gls{lstm} to produce an output of constant length (in their case, the hidden size of the encoder \gls{lstm}). The resulting vector is then fed into the decoder which is tasked with reconstructing the input sequence in reverse order. Here, the decoder can be configured to either be \textit{conditioned} or \textit{unconditioned}. A conditioned decoder uses the output of the last \gls{lstm} stage as input for the next stage. An unconditioned decoder uses the corresponding input token (ground truth) as input for the next stage. The latter practice is also called \textit{teacher forcing}. \par
The second unsupervised task to train the \gls{lstm} consists of predicting multiple future video frames. For this, again two consecutive \gls{lstm} networks are used: an encoder and a predictor \gls{lstm}. The first network is fed the frame representation of part of a short video and again produces a fixed sized output vector to be used by the predictor \gls{lstm}. The second \gls{lstm} is then tasked with producing the remaining frames. Same as with the auto-encoder, the predictor \gls{lstm} can either be conditioned or unconditioned. \par
The authors then propose a composite model as can be seen in figure \ref{fig:stateofart:unsupervised_lstm_composite} where both proxy tasks, reconstructing the input and predicting the future, are combined to produce a single model.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{img/composite_model_original.png}
	\caption{Composite model for input reconstruction and future prediction \cite{unsupervised_learning_lstms}}
	\label{fig:stateofart:unsupervised_lstm_composite}
\end{figure}

The pre-trained models are then fine-tuned for their classification task on the mentioned training datasets. With the pre-trained and fine-tuned composite model, the authors achieved an absolute increase of 1.3\% accuracy for both the UCF-101 and the HMDB-51 datasets over a conventional \gls{lstm} classifier as can be seen in table \ref{table:stateofart:unsupervised_learning_results}.

\begin{table}[]
	\centering
	\begin{tabular}{l c c c}
		\thead{Model} & \thead{UCF-101 \\ RGB} & \thead{UCF-101
\\ 1-frame flow} & \thead{HMDB-51 \\
RGB} \\ \hline
		\midrule
		Single Frame & 72.2 & 72.2 & 40.1 \\
		\midrule
		LSTM classifier & 74.5 & 74.3 & 42.8 \\
		\midrule
		Composite LSTM
Model + fine-tuning & 75.8 & 74.9 & 44.1 \\
	\end{tabular}
	\caption{Summary of results on Action Recognition \cite{unsupervised_learning_lstms}}
	\label{table:stateofart:unsupervised_learning_results}
\end{table}

For our thesis, we used the same auto-encoder and composite model for pre-training as explained in sections \ref{sec:experiments:lstm:auto_encoder} and \ref{sec:experiments:lstm:composite}.

\subsection{Unsupervised Pre-training of a Deep LSTM-based Stacked Autoencoder for Multivariate Time Series Forecasting Problems} \label{sec:stateofart:unsupervised_learning_lstms_timeseries}

In their 2019 paper \cite{unsupervised_learning_lstms_timeseries}, Alaa Sagheer et al. explore the benefits of unsupervised pre-training using stacked \gls{lstm} auto encoders with subsequent supervised fine-tuning. Their goal was to improve the prediction capabilities for \gls{mts} problems. In their previous paper \cite{dlstm_time_series_forecasting}, the authors showed the effectiveness of \gls{dlstm} based models for \gls{mts} prediction tasks. In their 2019 paper, they showed the improvements resulting from pre-training when compared to an initial random initialization of weights when working with \gls{dlstm} models. Compared to shallow \gls{lstm} networks, \gls{dlstm} networks contain multiple layers of \gls{lstm} cells stacked on each other. Information travels the network from left to right and from bottom to top as is depicted in figure \ref{fig:stateofart:unsupervised_learning_with_lstms_dlstm}. 

\begin{figure}[]
	\centering
	\includegraphics[width=1.0\textwidth]{img/multi_layer_lstm.png}
	\caption{Data flow in a three layered \gls{lstm} network.}
	\label{fig:stateofart:unsupervised_learning_with_lstms_dlstm}
\end{figure}

For pre-training the network, the authors use a \gls{lstmsae} model. In contrast to a conventional auto encoder like described in \ref{sec:backgrund:autoencoder}, a stacked auto encoder model uses multiple encoder and decoder layers as can be seen in figure \ref{fig:stateofart:unsupervised_learning_dlstm_mts_lstmsae}. The different encoder layers are trained individually in a multi phased training procedure: train the first auto encoder layer like a conventional \gls{lstmae} with the target being the original input data. Cut the decoder part of the first \gls{lstmae}. When training the second \gls{lstmae}, the input is encoded by both the encoder of the first and second \gls{lstmae} block and then decoded only by the decoder of the second \gls{lstmae}. The target data for training the second \gls{lstmae} is again the original input, and not the reconstructed input of the first \gls{lstmae}. The training process is depicted in figure \ref{fig:stateofart:unsupervised_learning_dlstm_mts_lstmsae}. This process is then repeated for arbitrarily many \gls{lstmae} layers. The authors tried both one and two stacked layers of \gls{lstmae}. The trained encoder blocks are then used to initialize a multi layered \gls{dlstm}.

\begin{figure}[]
	\centering
	\includegraphics[width=0.9\textwidth]{img/stacked_lstm_ae.png}
	\caption{Layer-wise pre-training of \gls{lstmsae} model. \cite{unsupervised_learning_lstms_timeseries}}
	\label{fig:stateofart:unsupervised_learning_dlstm_mts_lstmsae}
\end{figure}

To complete the training phase, the parameters of the pre-trained \gls{dlstm} are then fine-tuned in a supervised fashion. This is done by adding an output layer which produces values of the dimension of labels of the training set used and of the variables to be predicted. In the case of the authors, the output layer was a single neuron to predict a single variable. For supervised fine-tuning and validation, they used two datasets:

\begin{enumerate}
	\item \textit{The capital bike sharing dataset}
	\item \textit{The PM2.5 concentration in the air of CHINA dataset}
\end{enumerate}

For the first dataset, the model tried to predict how many bikes will be rented on a particular day based on parameters like \textit{Season, Holiday, Weekday, Working day} etc. For the second dataset, the task was to predict PM2.5 concentrations in the air for various Chinese cities based on parameters like \textit{Dew Point, Temperature, Pressure, Combined Wind Direction} etc. \par

As metric of performance, the authors used \gls{rmse}, \gls{mae} and \gls{smape} (lower is better) which all describe the difference between predicted value and observed value. The results of evaluation with both data sets can be seen in tables \ref{table:stateofart:unsupervised_learning_lstms_timeseries_results1} and \ref{table:stateofart:unsupervised_learning_lstms_timeseries_results2}.

\begin{table}[h]
	\begin{tabular}{l c c c c c c c}
		\thead{Model} & \thead{No. of \\ hidden
layer} & \thead{Dropout} & \thead{lag} & \thead{batch} & \thead{RMSE} & \thead{MAE} & \thead{SMAPE} \\ \hline
		\midrule
		DLSTM & 1 & 0.4 & 20 & 146 & 52.062 & 32.468 & 12.088 \\
		DLSTM & 2 & 0.3 & 25 & 219 & 49.811 & 31.524 & 12.183 \\
		LSTM-SAE & 1 & 0.1 & 30 & 219 & 49.389 & 32.192 & 13.878 \\
		LSTM-SAE & 2 & 0.1 & 30 & 73 & 46.927 & 30.041 & 11.646 \\
	\end{tabular}
	\caption{The results of \gls{dlstm} and \gls{lstmsae} using data set 1 \cite{unsupervised_learning_lstms_timeseries}}
	\label{table:stateofart:unsupervised_learning_lstms_timeseries_results1}
\end{table}

\begin{table}[h]
	\begin{tabular}{l c c c c c c c}
		\thead{Model} & \thead{No. of \\ hidden
layer} & \thead{Dropout} & \thead{lag} & \thead{batch} & \thead{RMSE} & \thead{MAE} & \thead{SMAPE} \\ \hline
		\midrule
		DLSTM & 1 & 0.2 & 30 & 60 & 23.993 & 12.124 & 10.919 \\
		DLSTM & 2 & 0.2 & 30 & 73 & 23.750 & 12.452 & 12.181 \\
		LSTM-SAE & 1 & 0.1 & 30 & 219 & 23.907 & 12.509 & 11.052 \\
		LSTM-SAE & 2 & 0.3 & 25 & 146 & 24.041 & 12.060 & 9.864 \\
	\end{tabular}
	\caption{The results of \gls{dlstm} and \gls{lstmsae} using data set 2 \cite{unsupervised_learning_lstms_timeseries}}
	\label{table:stateofart:unsupervised_learning_lstms_timeseries_results2}
\end{table}

The results in tables \ref{table:stateofart:unsupervised_learning_lstms_timeseries_results1} and \ref{table:stateofart:unsupervised_learning_lstms_timeseries_results2} show that unsupervised pre-training improved final accuracy and led to better and faster convergence. \par

For this thesis, we used only a single \gls{lstmae} network, but with three layers of \gls{lstm} cells making it a \gls{dlstm} for both encoder and decoder.

%\begin{itemize}
%	\item Findings: What do they claim (main findings)
%	\item Data: What data set they are using
%	\item Methods: Which methods did they use?
%	\item Reproducibility: Is it possible to reproduce the results? (e.g., is the data available? are all parameter settings provided? Is source code provided?)
%	\item Relevance (How relevant is it for your work)
%\end{itemize}
%
%
%In the last paragraph explain how your work differs from the existing works.



\newpage
