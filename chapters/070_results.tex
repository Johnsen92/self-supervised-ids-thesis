\chapter{Results} \label{sec:results}

In this chapter, we discuss results and try to explain them based on neuron activation and \gls{pd} plots.

\begin{itemize}
	\item maximum accuracy with 0-90-10 pre-sup-val training
	\item comparison between pretraining accuracy with different proxy tasks for 10-80-10 pre-sup-val training
	\item comparison between pretraining accuracy with different proxy tasks for 1-89-10 pre-sup-val training
	\item comparison between pretraining accuracy with different proxy tasks for subset 10\_flows subset pre-sup-val training
	\item comparison of performance improvements for different amounts of supervised training
	\item comparison of performance improvements for different compositions of pretraining data
	\item comparison between multiple datasets
	\item comparison to orthogonal initialization/random initialization/0-initialization
	\item comparison of validation loss and accuracy convergence for different pretraining tasks
\end{itemize} 

\section{Long Short-Term Memory Network} \label{sec:results:lstm}

\begin{table}[htb]
	\centering
	\scalebox{0.77}{
	\begin{tabular}{@{}ccccccc@{}}
		\toprule
		&  NONE &  AUTO &  ID &  COMPOSITE &  OBSCURE &  PREDICT \\
		\midrule
		Hyperparameters &  &  &  &  &  &  \\
		Epochs Supervised &  50 &  50 &  50 &  50 &  50 &  50 \\
		Epochs Pretraining &  50 &  10 &  10 &  10 &  10 &  10 \\
		Batch size &  128 &  128 &  128 &  128 &  128 &  128 \\
		Proxy task &  NONE &  AUTO &  ID &  COMPOSITE &  OBSCURE &  PREDICT \\
		Pretraining percentage &  0.00 \% &  80.00 \% &  80.00 \% &  80.00 \% &  80.00 \% &  80.00 \% \\
		Training percentage &  10.00 \% &  10.00 \% &  10.00 \% &  10.00 \% &  10.00 \% &  10.00 \% \\
		Validation percentage &  10.00 \% &  10.00 \% &  10.00 \% &  10.00 \% &  10.00 \% &  10.00 \% \\
		Specialized subset &   &   &   &   &   &   \\
		Learning rate &  0.001 &  0.001 &  0.001 &  0.001 &  0.001 &  0.001 \\
		Random Seed &  500 &  500 &  500 &  500 &  500 &  500 \\
		\\
		Modelparameters &  &  &  &  &  &  \\
		Hidden size &  512 &  512 &  512 &  512 &  512 &  512 \\
		\# Layers &  3 &  3 &  3 &  3 &  3 &  3 \\
		\\
		Training metrics &  &  &  &  &  &  \\
		Best epoch &  49 &  46 &  48 &  47 &  49 &  47 \\
		Time to best epoch &  1h 34m &  1h 59m &  1h 43m &  3h 13m &  1h 40m &  1h 35m \\
		\\
		Performance metrics &  &  &  &  &  &  \\
		Accuracy &  99.632 \% &  99.630 \% &  99.727 \% &  99.733 \% &  99.620 \% &  99.705 \% \\
		False alarm rate &  0.289 \% &  0.337 \% &  0.229 \% &  0.229 \% &  0.384 \% &  0.278 \% \\
		Missed alarm rate &  1.172 \% &  1.129 \% &  0.854 \% &  0.829 \% &  1.123 \% &  0.892 \% \\
		Detection rate &  98.828 \% &  98.871 \% &  99.146 \% &  99.171 \% &  98.877 \% &  99.108 \% \\
		Precision &  99.711 \% &  99.663 \% &  99.771 \% &  99.771 \% &  99.616 \% &  99.722 \% \\
		Specificity &  99.903 \% &  99.887 \% &  99.923 \% &  99.923 \% &  99.871 \% &  99.907 \% \\
		Recall &  99.711 \% &  99.663 \% &  99.771 \% &  99.771 \% &  99.616 \% &  99.722 \% \\
		F1-Measure &  99.711 \% &  99.663 \% &  99.771 \% &  99.771 \% &  99.616 \% &  99.722 \% \\
		\bottomrule
	\end{tabular}}
	\caption{Results for experiments 2.1.1 - 2.1.6 on dataset CIC-IDS-2017}
\end{table}


\subsection{Identity Function} \label{sec:results:lstm:identity}

\begin{itemize}
	\item camparison identity function with both datasets 
\end{itemize} 

\subsection{Predict Packet} \label{sec:results:lstm:predict_packet}

\subsection{Mask Features} \label{sec:results:lstm:mask_feature}

\begin{itemize}
	\item compare differences strategies for masking features
\end{itemize} 

\subsection{Mask Packets} \label{sec:results:lstm:mask_packet}

\subsection{Auto-Encoder} \label{sec:results:lstm:auto_encoder}

\begin{itemize}
	\item compare differences between conditioned and unconditioned model
\end{itemize} 

\subsection{Composite model} \label{sec:results:lstm:composite}

\begin{itemize}
	\item compare differences between conditioned and unconditioned model
\end{itemize} 

\begin{itemize}
	\item provide data for maximum results including class stats for both datasets to establish a feel for the maximally possible accuracy with supervised training and 90\% of data
	\item show results for different amounts of supervised data and discuss results between different proxy tasks by showing loss progression and validation accuracy over training time and comparing class stats
	\item highlight the improvement in accuracy when comparing to supervised training only
	\item look closely at differences in loss progression and validation accuracy over time between different proxy tasks
\end{itemize}

\section{Transformer Network} \label{sec:results:transformer}

\subsection{Mask Features} \label{sec:results:transformer:mask_features}

\subsection{Autoencoder} \label{sec:results:transformer:autoencoder}

\subsection{Mask Packet} \label{sec:results:transformer:mask_packet}

\section{Explainability}

\begin{itemize}
	\item close look at differences in performance for different attack classes
	\item partial dependency plots
	\item neuron activation
	\item \gls{dlstm}s and transformer encoder already very effective, so improvement is hard
\end{itemize}

\newpage
