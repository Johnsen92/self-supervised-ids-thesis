\chapter{Results} \label{sec:results}

In this chapter, we present the results from experiments proposed in the previous chapter. Like with the experiments section, we will be looking at the results from \gls{lstm} and the transformer model independently. An in-depth look into the datasets and the workings of the differently trained models based on \gls{pd} plots, neuron data plots and the outputs of a fitted \gls{dtc} follow in the section \ref{sec:results:explainability}.wweeeeessssssssssssssssesaq

\section{Long Short-Term Memory Model} \label{sec:results:lstm}

As a baseline, we look at results where the model has been trained in a purely supervised fashion with different amounts of data of the two datasets. The results are comparable to previous experiments with deep neural networks on these datasets \cite{fog_based_detection_survey_2020} and even slightly better in some instances. Looking at tables \ref{table:results:lstm:stats_flows_supervised}, \ref{table:results:lstm:stats_flows15_supervised} we can already see that very little supervised data is needed to achieve fairly high accuracy. For the \gls{lstm} model, going from 90\% of training data (exp. 1.1.1 and 1.2.1) to 10\% (exp. 2.1.1 and 2.4.1) only amounts to an absolute drop of 0.164\% and 0.276\% accuracy for CIC-IDS2017 and UNSW-NB15 datasets respectively. Most astounding are also the results when dropping from millions of records when training with 90\% of the datasets to just the specialized subsets containing a couple of hundred entries in total and only 10 records of each attack class. Withholding most of labeled data in the datasets, this constraint only amounts to an absolute accuracy decrease of 4.114\% and 1.176\% for datasets CIC-IDS2017 and UNSW-NB15 respectively.
While this is fairly pleasant in general, it means that results will be harder to improve as any benefit pre-training provides might be overshadowed by the effectiveness of supervised training, even with very little data. \par

\input{results/results/rn500/lstm/tables/flows_supervised/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_supervised/class_comparison_ALL}

Next, we will be looking at results for pre-training with the different proxy tasks and different amounts of data used for supervised finetuning. Tables \ref{table:results:lstm:stats_flows_10}, \ref{table:results:lstm:stats_flows_1} and \ref{table:results:lstm:stats_flows_subset} show results for experiments 2.1.1 - 2.1.6, 2.2.1 - 2.2.6 and 2.3.1 - 2.3.6 on dataset CIC-IDS2017 conducted with 10\%, 1\% and only a very small fraction of data as defined in subset CIC17\_10. Looking at the performance metrics, we can see that there is some variance in the resulting data. The NumPy and PyTorch random seeds are the same for all experiments which means that pre-training, supervised finetuning and validation have been conducted with the exact same subsets of the original dataset which means that differences in results can only come from pre-training with different proxy tasks. This establishes the fact that pre-training in general, and also different methods of pre-training have an effect on final performance. Starting with table \ref{table:results:lstm:stats_flows_10}, we can see that pre-training with some proxy tasks improves performance while others have almost no effect or even a negative effect. \par

\input{results/results/rn500/lstm/tables/flows15_supervised/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_supervised/class_comparison_ALL}

For accuracy, the highest positive delta 0.101\% in experiments 2.1.1-6 in table \ref{table:results:lstm:stats_flows_10} can be observed for pre-training with the COMPOSITE proxy task \ref{sec:experiments:lstm:composite} closely followed by pre-training with the ID proxy task \ref{sec:experiments:lstm:identity} with a delta of 0.095\%. The highest negative delta in accuracy, -0.010\%, can be observed for the OBSCURE feature proxy task \ref{sec:experiments:lstm:obscure}. It should be noted, that for detection rate, the highest delta is 0.343\% also occurring after COMPOSITE pre-training. This shows that the improvement in accuracy stems from improved attack detection capability achieved through pre-training. \par

\input{results/results/rn500/lstm/tables/flows_10/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_10/class_comparison_ALL}

Looking at table \ref{table:results:lstm:stats_flows_1} we can inspect for which attack categories improvement is most salient. When comparing training with supervised methods only in experiment 2.1.1 with the on the COMPOSITE proxy task pre-trained model in experiment 2.1.6 we can see major improvements for detection of FTP-Patator brute force attacks. Accuracy jumped from 72\% for supervised only training to 92.308\% for the COMPOSITE trained model and even 96.154\% for the PREDICT proxy task, constituting a positive delta of 24.154\%. For experiment 2.1.4, pre-training with the auto encoder task, the accuracy for detection of the FTP-Patator attack category dropped to 3.846\%. Such high variance in results shows again that the \gls{lstm} model is susceptible to different pre-training strategies. Other attack classes which have seen improvement in detection accuracy are port scans with firewall on and off (\#11-12) with positive deltas of 2.702\% and 0.606\% respectively and infiltration (\#9) and SSH-Patator (\#2) with deltas of 1.269\% and 1.185\%. \par

\input{results/results/rn500/lstm/tables/flows_1/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_1/class_comparison_ALL}

Looking at results for experiments 2.2.1-6 \ref{table:results:lstm:stats_flows_1} finetuned with 1\% of the CICIDS-2017 dataset - \textit{ceteris paribus} - the maximum delta in accuracy increased to 0.178\%, which in this iteration is observed after pre-training with the PREDICT proxy task \ref{sec:experiments:lstm:predict_packet}. The positive delta for the COMPOSITE proxy tasks increased to 0.136\% and the delta for the ID proxy task remained almost the same at 0.094\%. PREDICT, ID and COMPOSITE proxy tasks have shown improvements in accuracy and performance overall for experiments 2.1.1-6 and 2.2.1-6, but the pattern breaks down when looking at experiments 2.3.1-6 with finetuning performed with subset CIC17\_10 where improvement is now only present for pre-training with the ID proxy task where the positive deltas in accuracy and detection rate have increased to 0.594\% and 3.602\% respectively. All other pre-training resulted in strongly reduced performance most salient with the OBSCURE proxy task with a negative delta in accuracy of -2.327\%. \par 

\input{results/results/rn500/lstm/tables/flows_subset/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_subset/class_comparison_ALL}

It shall be noted that training with this little data entails a high variability in validation accuracy and loss over the course of training. In our case, validation accuracy was tested only every 6 epochs of training for the overall 600 training epochs to keep training times reasonable. Higher accuracy scores might have occurred during
epochs in which validation accuracy was not tested before the model started overfitting. The same caveat must be stated for experiments 2.2.1-6 in table \ref{table:results:lstm:stats_flows_1} or 2.5.1-6 in table \ref{table:results:lstm:stats_flows15_1} where validation accuracy was tested only every second epoch during training, but training for these experiments was much more stable in general and 1/2 is a much higher chance of catching the highest result than 1/6. \par 

\input{results/results/rn500/lstm/tables/flows15_10/stats_comparison_ALL}

It also shall be noted that for training with the CIC17\_10 and UNSW15\_10 the ratio of samples between categories has changed when compared to the original dataset or the stratified sampled 10\% and 1\% subsets. For CIC17\_10 and UNSW15\_10, the same amount of samples per category are included. This does not impact the comparison between pre-trained and non-pre-trained models but takes from the comparability between results of experiments 2.3.1-6 in table \ref{table:results:lstm:stats_flows_subset} and the results of experiments 2.1.1-6 in table \ref{table:results:lstm:stats_flows_10} and 2.2.1-6 in table \ref{table:results:lstm:stats_flows_1}. \par
A similar pattern emerges from the results of tests on the UNSW-NB15 dataset. For experiments 2.4.1-6 in table \ref{table:results:lstm:stats_flows15_10} finetuned with 10\% of the dataset minor improvement can again be observed for the pre-trained models. The highest positive delta of 0.086\% occurred after pre-training with de identity function proxy task in experiment 2.4.5 when comparing to the purely supvervised training in experiment 2.4.1. Training with other proxy tasks shows comparably minor improvements to accuracy. \par

\input{results/results/rn500/lstm/tables/flows15_10/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_1/stats_comparison_ALL}

Looking at class specific accuracy in table \ref{table:results:lstm:class_flows15_10} we can see that contrary to the results from experiments 2.1.1-6 on dataset CIC-IDS2017, here the increase in overall accuracy stems from minor improvements on benign flow classification i.e. an increase in specificity rather than an increase in detection rate. In fact, detection rate drops for all pre-trained models in experiments 2.4.1-6 most salient in results from experiment 2.4.6 trained on the COMPOSITE proxy-task where detection rate drops by 1.273\% but specificity increased by 0.084\% when compared to supervised only training in experiment 2.4.1 resulting in an improvement in accuracy of 0.037\% due to 96.64\% of the UNSW-NB15 dataset being benign flows. \par

\input{results/results/rn500/lstm/tables/flows15_1/class_comparison_ALL}

The maximum delta between supervised only trained and pre-trained models increases to 0.137\% in experiments 2.5.1-6 in table \ref{table:results:lstm:stats_flows15_1}, this time occurring for proxy task AUTO with all other proxy tasks also showing minor improvements in accuracy. Just looking at experiment 2.5.4, contrary to experiments 2.4.1-6 in table  \ref{table:results:lstm:stats_flows15_10}, specificity and detection rate increased slightly when compared to baseline experiment 2.5.1. While in experiments 2.4.1-6 in table \ref{table:results:lstm:stats_flows15_10}, all pre-trained models show a decrease in detection rate, in experiments 2.5.1-6 in table  \ref{table:results:lstm:stats_flows15_1} all pre-training methods except the COMPOSITE proxy-task result in an improved detection rate. \par

\input{results/results/rn500/lstm/tables/flows15_subset/stats_comparison_ALL}

The pattern of improved accuracy breaks again when looking at finetuning with the specialized subset UNSW15\_10 in table \ref{table:results:lstm:stats_flows15_subset} for experiments 2.6.1-6. Here, almost no improvement is measurable and for the COMPOSITE proxy task accuracy even dropped by 0.168\% when for finetuning with 1\% and 10\% it showed slight improvement. The only increases in accuracy are measurable for the PREDICT and AUTO proxy tasks with very minor accuracy increases of 0.005\% and 0.010\% with all other pre-training methods resulting in lower accuracy scores best represented by experiment 2.6.6 with the COMPOSITE proxy-task where accuracy dropped by 0.168\%. \par

\input{results/results/rn500/lstm/tables/flows15_subset/class_comparison_ALL}

\FloatBarrier

\section{Transformer Model} \label{sec:results:transformer}

\input{results/results/rn500/transformer/tables/flows_supervised/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_supervised/class_comparison_ALL}

The transformer model without pre-training produces state of the art results, similar to experiments with the \gls{lstm} model but  performs slightly worse: For 90\% of data for supervised training only 0.138\% and 0.187\% absolute difference for CIC-IDS2017 and UNSW-NB15 datasets respectively \ref{table:results:transformer:stats_flows_supervised}.
For UNSW-NB15 for 10\% and 1\% finetuning no improvements were achieved, except for a 0.03\% plus in accuracy for 3.5.4 \ref{table:results:transformer:stats_flows15_1} which is most likely just variance. \par

\input{results/results/rn500/transformer/tables/flows_10/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_10/class_comparison_ALL}

For experiments 3.6.1-4 \ref{table:results:transformer:stats_flows15_subset} on dataset UNSW-NB15 with subset UNSW15\_10 both the AUTO proxy task yielded minor improvements of 0.295\% accuracy but with the MASK proxy task the model accuracy fell by 1.213\% as the model defaulted to guessing negative i.e. detection rate: 0\%. The accuracy improvements in experiment 3.6.4 stems from a major increase in detection rate of an absolute 29.424\% when compared to experiment 3.6.1 i.e. performance of the model without pre-training. \par
Surprisingly, the MASK proxy task yielded the worst results overall when looking at accuracy as it dropped for 4/6 experiment series even though it is the most similar proxy task used compared to the one used in Google BERT.


\input{results/results/rn500/transformer/tables/flows_1/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_1/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_subset/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_subset/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_10/stats_comparison_ALL}

\clearpage

\input{results/results/rn500/transformer/tables/flows15_10/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_1/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_1/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_subset/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_subset/class_comparison_ALL}

\FloatBarrier

\section{Explainability} \label{sec:results:explainability}

In this section we are trying to analyze the mechanisms involved in generating the results in the previous section and analyze the datasets used. As with machine learning in general, complete explainability is hard to achieve, hence the following attempts are our best efforts to make sense of the results. We try to answer the questions

\begin{itemize}
	\item Did the pre-training have any effect on the model behavior and if yes: how did predictions change?
	\item Which features where relevant for classification? 
	\item Why did the model perform so well, even with very little training data?
	\item Where the datasets suited to be used for the conducted experiments?
\end{itemize}

To answer these questions, we conducted a series of tests including plotting neuron activation data, a \gls{pd} analysis, and fitting a \gls{dtc} to discern the value thresholds of features which lead to correct classifications. 

\subsection{Neuron Activation Plots}

To answer the question whether pre-training had any effect at all on the behavior of the \gls{lstm} model, we looked at neuron activations of the last stage of the \gls{lstm}, i.e. the hidden state of the last stage of the last layer of the model after having processed the whole data sequence, after pre-training and after fine-tuning. The assumption was, that if the information the model learned during pre-training proofed useful for classification the neurons which were activated after pre-training would still be activated after fine-tuning. The same neuron activations not being present after fine-tuning does however not mean, that pre-training had no positive effect on classification. As we are only looking at the last stage of the last layer of the \gls{lstm}, the learned information might have been used in a previous layer of the \gls{lstm} to influence later layers towards a more accurate classification which would not show in these results. To have some form of quantitative metric, we calculated the \gls{mae} between the activated neurons after pre-training, i.e. neurons unequal to zero, and the same neurons after fine-tuning. Instances where the neuron was not activated i.e. activation level $< 0.1$ in both cases are omitted from the plots (but not the MAE calculation) to increase readability.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\linewidth]{results/results/rn500//lstm/plots/neuron/flows_ID/latest/11_PortScan_-_Firewall_off.png}
	\caption{Neuron activation plot comparing neuron activations of the latest stage of the \gls{lstm} model (after the last packet in the sequence has been processed) after pre-training with the ID proxy task and after fine-tuning with flows classified as \textit{PortScan - Firewall off} in the CIC-IDS2017 dataset.}
	\label{fig:results:lstm:neuron:cic17_id_port_scan_firewall_off}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\linewidth]{results/results/rn500//lstm/plots/neuron/flows_ID/latest/9_Infiltration.png}
	\caption{Neuron activation plot comparing neuron activations of the latest stage of the \gls{lstm} model (after the last packet in the sequence has been processed) after pre-training with the ID proxy task and after fine-tuning with flows classified as \textit{Infiltration} in the CIC-IDS2017 dataset.}
	\label{fig:results:lstm:neuron:cic17_id_infiltration}
\end{figure}

\FloatBarrier

\subsection{Partial Dependency Plots}

We started with the \textbf{\gls{pd}} analysis on some of the features
to discern their effect on the classification of each attack type in the dataset. 
The resulting \glspl{pdp} were generated with the models from experiments \ref{table:results:lstm:stats_flows_subset},
\ref{table:results:lstm:stats_flows15_subset}, \ref{table:results:transformer:stats_flows_subset}, \ref{table:results:transformer:stats_flows15_subset} because we assumed that pre-training has the most impact on models trained on very little labeled training data. The features we looked at where \textit{source port number}, \textit{destination port number}, \textit{protocol identifier} and \textit{packet length}. \par
For both datasets, we generated plots to inspect how the predictions of the model change after altering the value of the feature (\textit{ceteris paribus}). The value range of each feature was withing the highest and lowest occurring value of said feature in the dataset. The colored graph lines represent the predictions, i.e. the activation level of the output neuron of the model trained with the respective proxy task denoted in the graph legend in the top left corner.
The histogram together with the right-hand side vertical axis describes the number of flows in the dataset, i.e. in the 90\% used for training, with this feature value. 
Examples of these plots can be seen in figures \ref{fig:results:lstm:pdp:cic17_packet_length_ssh_patator} and \ref{fig:results:lstm:pdp:cic17_source_port_ssh_patator}. After we generated the data for each tuple of ($dataset$ x $model$ x $feature$ x $attack type$) we generated plots to compare the results of the different models yielding a total of 160 plots. Most of them are of little interest as often the inspected feature had no or very little impact on the classification of the attack type. \par 

\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\linewidth]{results/results/rn500/lstm/plots/pdp/flows_NONE_PREDICT_OBSCURE_AUTO_ID_COMPOSITE/Packet_Length_2_SSH-Patator_3}
	\caption{Partial dependency plot between the packet length and classification of SSH Patator attacks in the CIC-IDS2017 dataset.}
	\label{fig:results:lstm:pdp:cic17_packet_length_ssh_patator}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\linewidth]{results/results/rn500/lstm/plots/pdp/flows_NONE_PREDICT_OBSCURE_AUTO_ID_COMPOSITE/Source_port_2_SSH-Patator_0.png}
	\caption{Partial dependency plot between the source port and classification of SSH Patator attacks in the CIC-IDS2017 dataset.}
	\label{fig:results:lstm:pdp:cic17_source_port_ssh_patator}
\end{figure}

What is immediately apparent from the the plots is that even though the final metrics of the experiments with different proxy tasks where similar, the general behavior of the models trained with different proxy tasks is effected by pre-training. In the range where the actual feature values relevant for classification of specific attack categories lay the models behave very similar which leads to correct classification. An example of this can be seen in figure \ref{fig:results:lstm:pdp:cic17_source_port_ssh_patator} where, deducing from the plots and from a later inspection of the decision tree thresholds of the fitted \gls{dtc}, the source port is an important feature when it comes to classifying \textit{SSH Patator} attacks. Varying this feature leads to different behavior of the differently trained models, but in the value range the feature actually takes on in the dataset, i.e. the relevant section, the models behave very similar. We will take a deeper look into this issue later in the section when we look at the structure of the decision tree used to classify each attack category. If no such clear pattern in the plot emerges like e.g. in plot \ref{fig:results:lstm:pdp:cic17_packet_length_ssh_patator}, it means that there the value of the feature has little impact on the classification of the attack type. Here the dependency of the classification of the \textit{SSH Patator} attack category on the feature \textit{packet length} is inspected.

\FloatBarrier

\subsection{Decision Tree Classifier}

Due to the high accuracy rates the models achieved with very little training data i.e. the specialized subsets as specified in section \ref{sec:methodology:subsets}, we suspected that the datasets might simply be easy to classify. To get additional information on how the models might classify the data and discern benign from attack traffic, we fitted a \gls{dtc} on both used datasets and plotted the resulting decision trees. In table \ref{fig:results:dtc:depth_analysis} we evaluated the accuracy of the decision tree classifier with different maximum depths when tasked with binary classification of the datasets. 

\input{results/results/rn500/dt/tables/flows_depth20/stats_comparison_ALL.tex}

\input{results/results/rn500/dt/tables/flows15_depth16/stats_comparison_ALL.tex}

The accuracy scores listed in tables \ref{table:results:dt:stats_flows_depth20} and \ref{table:results:dt:stats_flows15_depth16} are based on per packet classification so they are not completely comparable to the results from the \gls{dnn} models as their validation metrics are based on the classification of the extracted flows and not packets. The results where generated with the exact same stratified training and validation splits as were used for training the \gls{dl} models to keep some comparability. The results where generated with a maximum depth of 20 and 16 for datasets CIC-IDS2017 and UNSW-NB15 respectively. With these parameters the \gls{dtc} achieves peak performance before it starts to overfit as depicted in table \ref{table:results:explainability:model_comparison}. This indicates that records of CIC-IDS2017 dataset might be slightly harder to classify. The high accuracy achieved by the \gls{dtc} on a packet basis without having any concept of flows or sequences might indicate that the datasets indeed might be simply very easy to classify. 

\begin{table}[!h]
	\centering
	\begin{tabular}{c|cc|cc}
		& \multicolumn{2}{l}{\thead{\textbf{CIC-IDS-2017}}} & \multicolumn{2}{l}{\thead{\textbf{UNSW-NB15}}} \\ \midrule
		\thead{\textbf{max. depth}} & \thead{\textbf{accuracy}}      & \thead{\textbf{fitting time}}     & \thead{\textbf{accuracy}}     & \thead{\textbf{fitting time}}   \\ \midrule
		1          & 82.8558\%     & 202.97s          & 98.7241\%    & 212.02s        \\
		2          & 82.8558\%     & 216.38s          & 98.7695\%    & 227.32s        \\
		3          & 89.3871\%     & 216.94s          & 98.8434\%    & 249.41s        \\
		4          & 90.9993\%     & 226.19s          & 98.8562\%    & 268.7s         \\
		5          & 91.6236\%     & 242.34s          & 98.9273\%    & 285.73s        \\
		6          & 93.3365\%     & 241.75s          & 99.0839\%    & 312.4s         \\
		7          & 93.6416\%     & 252.43s          & 99.1811\%    & 330.9s         \\
		8          & 96.4964\%     & 259.8s           & 99.2847\%    & 347.64s        \\
		9          & 96.9901\%     & 271.28s          & 99.3517\%    & 363.32s        \\
		10         & 97.3088\%     & 266.96s          & 99.3894\%    & 381.01s        \\
		11         & 97.6354\%     & 265.23s          & 99.4337\%    & 399.33s        \\
		12         & 97.7696\%     & 269.61s          & 99.4546\%    & 415.14s        \\
		13         & 97.9199\%     & 273.56s          & 99.4596\%    & 429.8s         \\
		14         & 98.0323\%     & 277.4s           & 99.4671\%    & 454.4s         \\
		15         & 98.0646\%     & 274.03s          & 99.4715\%    & 455.26s        \\
		16         & 98.0982\%     & 281.64s          & 99.472\%     & 453.49s        \\
		17         & 98.1472\%     & 274.37s          & 99.472\%     & 457.97s        \\
		18         & 98.1662\%     & 275.86s          & 99.468\%     & 461.35s        \\
		19         & 98.1792\%     & 276.26s          & 99.4609\%    & 457.48s        \\
		20         & 98.1928\%     & 288.68s          & 99.4531\%    & 457.15s        \\
		21         & 98.1906\%     & 297.94s          & 99.4447\%    & 475.83s        \\
		22         & 98.167\%      & 309.79s          & 99.4244\%    & 502.92s        \\
		23         & 98.1783\%     & 310.07s          & 99.4131\%    & 498.34s        \\
		24         & 98.1524\%     & 313.42s          & 99.4028\%    & 506.54s        \\
		25         & 98.154\%      & 312.18s          & 99.4058\%    & 486.02s       
	\end{tabular}
	\caption{Performance of \gls{dtc} for binary classification fitted on 90\% of data from the respective dataset with different maximum depth values. Accuracy was calculated on the remaining 10\% of data not used for fitting.}
	\label{fig:results:dtc:depth_analysis}
\end{table}

A more detailed comparison between results yielded from the \gls{dtc} and the \gls{dl} models without pre-training can be found in table \ref{table:results:explainability:model_comparison}, again with the caveat that the \gls{dtc} is evaluated on packets and not flows, hence shifting the ratio of attack to benign records slightly. The validation subset for CIC-IDS-2017 consisting of 2493032 packets and is composed of 82.86\% benign packets and 17.14\% attack packets. In flow representation, the dataset consists of
74.72\% benign and 25.28\% attack records. The validation subset for UNSW-NB15 consisting of 6228573 packets and is composed of 98.33\% benign packets and 1.67\% attack packets. In flow representation, the dataset consists of 
96.64\% benign and 3.36\% attack records. Only looking at accuracy, the \gls{dtc} performs similarly to the \gls{dl} models, but other metrics e.g. FAR and MAR, are considerably worse especially when comparing results for the cases where the models had access to 90\% of the datasets. 

\begin{table}[!h]
	\begin{tabular}{c|ccc|ccc}
		\thead{\textbf{Trained with}} & \multicolumn{3}{c}{\thead{\textbf{CIC-IDS-2017}}}    & \multicolumn{3}{c}{\thead{\textbf{UNSW-NB15}}}       \\ \midrule
		\thead{\textbf{90.00}}\%      & \thead{\textbf{LSTM}}      & \thead{\textbf{Transformer}} & \thead{\textbf{DTC*}}      & \thead{\textbf{LSTM}}      & \thead{\textbf{Transformer}} & \thead{\textbf{DTC*}}      \\ \midrule
		Accuracy     & 99.796 \% & 99.658 \%   & 98.193 \% & 98.930 \% & 98.743 \%   & 99.453 \% \\
		DR           & 99.306 \% & 98.884 \%   & 95.101 \% & 82.936 \% & 79.300 \%   & 82.125 \% \\
		Precision    & 99.885 \% & 99.762 \%   & 94.399 \% & 86.315 \% & 84.283 \%   & 84.628 \% \\
		Specificity  & 99.961 \% & 99.920 \%   & 98.832 \% & 99.517 \% & 99.457 \%   & 99.747 \% \\
		F1-Measure   & 99.595 \% & 99.321 \%   & 94.749 \% & 84.592 \% & 81.716 \%   & 83.358 \% \\
		FAR          & 0.115 \%  & 0.238 \%    & 5.601 \%  & 13.685 \% & 15.717 \%   & 15.372 \% \\
		MAR          & 0.694 \%  & 1.116 \%    & 4.899 \%  & 17.064 \% & 20.700 \%   & 17.875 \% \\ \midrule
		\thead{\textbf{10.00\%}}      & \thead{\textbf{LSTM}}      & \thead{\textbf{Transformer}} & \thead{\textbf{DTC*}}      & \thead{\textbf{LSTM}}      & \thead{\textbf{Transformer}} & \thead{\textbf{DTC*}}      \\ \midrule
		Accuracy     & 99.632 \% & 99.448 \%   & 97.849 \% & 98.654 \% & 98.545 \%   & 99.322 \% \\
		DR           & 98.828 \% & 98.576 \%   & 94.507 \% & 80.684 \% & 88.111 \%   & 76.731 \% \\
		Precision    & 99.711 \% & 99.236 \%   & 93.140 \% & 81.166 \% & 75.079 \%   & 82.350 \% \\
		Specificity  & 99.903 \% & 99.743 \%   & 98.547 \% & 99.313 \% & 98.928 \%   & 99.714 \% \\
		F1-Measure   & 99.268 \% & 98.905 \%   & 93.819 \% & 80.924 \% & 81.075 \%   & 79.441 \% \\
		FAR          & 0.289 \%  & 0.764 \%    & 6.860 \%  & 18.834 \% & 24.921 \%   & 17.650 \% \\
		MAR          & 1.172 \%  & 1.424 \%    & 5.493 \%  & 19.316 \% & 11.889 \%   & 23.269 \% \\ \midrule
		\thead{\textbf{1.00\%}}       & \thead{\textbf{LSTM}}      & \thead{\textbf{Transformer}} & \thead{\textbf{DTC*}}      & \thead{\textbf{LSTM}}      & \thead{\textbf{Transformer}} & \thead{\textbf{DTC*}}      \\ \midrule
		Accuracy     & 99.385 \% & 99.189 \%   & 96.874 \% & 98.305 \% & 98.328 \%   & 99.124 \% \\
		DR           & 98.408 \% & 98.640 \%   & 92.718 \% & 78.846 \% & 84.212 \%   & 69.783 \% \\
		Precision    & 99.153 \% & 98.161 \%   & 89.513 \% & 74.673 \% & 72.790 \%   & 76.189 \% \\
		Specificity  & 99.716 \% & 99.374 \%   & 97.739 \% & 99.019 \% & 98.845 \%   & 99.626 \% \\
		F1-Measure   & 98.779 \% & 98.400 \%   & 91.087 \% & 76.703 \% & 78.086 \%   & 72.846 \% \\
		FAR          & 0.847 \%  & 1.839 \%    & 10.487 \% & 25.327 \% & 27.210 \%   & 23.811 \% \\
		MAR          & 1.592 \%  & 1.360 \%    & 7.282 \%  & 21.154 \% & 15.788 \%   & 30.217 \%
	\end{tabular}
	\caption{Comparison between model performances without pre-training for 90\%, 10\% and 1\% of training data with random seed 500 and stratified sampling. \gls{dtc} performance is only partly comparable as it operates on packets and not flows.}
	\label{table:results:explainability:model_comparison}
\end{table}

Next we tried to fit a \gls{dtc} on subsets of the datasets containing only benign flows and flows of one attack category to obtain thresholds for the feature values which lead to the correct classification of the different attack types. We used a maximum depth of 5 as a trade-off between accuracy and readability of the resulting decision trees. An overview of all the results can be seen in tables \ref{table:results:dtc:flows} and \ref{table:results:dtc:flows15}. 

\input{results/results/rn500/dataset/dt_flows_md5_summary}

\input{results/results/rn500/dataset/dt_flows15_md5_summary}

The resulting trees can be inspected to discern which features are important for classifying the different attack types. E.g. in figure \ref{fig:results:dtc:cic2017:ssh_patator} the decision tree for the classification of \textit{SSH-Patator} flows in the CIC-IDS-2017 dataset is depicted. The tree is to be interpreted as following: Each node constitutes a binary split in the data. The first line in each node contains a threshold of a feature value e.g. \textit{Destination port <= 22.5} in the root node. All samples in the branch to the left fulfill the criterion, all samples to the right don't. The label \textit{gini} indicates the quality of the split measured with the \textit{Gini impurity}. The label \textit{samples} states how many samples have been passed on to this node. The label \textit{value} the current guess of the classifier for how the current set is comprised. The first value constitutes a guess of the \gls{dtc} on how many \textit{benign} samples are still in the set and second value of how many \textit{attack} samples remain. The proportion of remaining \textit{benign} and \textit{attack} values is also depicted in the color of the node. The label \textit{class} indicates the best guess of the \gls{dtc} at this point in the tree. As the graphical representation is too small to be printed if all leafs of the tree are present, we present only the textual tree representation as generated by the \textit{scikit-learn} \gls{ml} python library \cite{sklearn} for those instances. A list of all generated trees can be found in the appendix in section \ref{label}. \par

\begin{figure}[]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/dataset/flows/plots/dt_5_2_SSH-Patator.png}
	\caption{Decision tree yielded from a \gls{dtc} fitted to the a subset of the CIC-IDS2017 dataset (99.329\% benign samples, 0.671\% attack samples) containing only benign flows and attack flows of category \textit{SSH-Patator}. The \gls{dtc} achieved an accuracy of 99.781\%.}
	\label{fig:results:dtc:cic2017:ssh_patator}
\end{figure}

Tables \ref{table:results:explainability:feature_importance_datasets_90}, \ref{table:results:dtc:fimp_flows} and \ref{table:results:dtc:fimp_flows15} constitute a closer look at the importance of different features for binary classification. The values in the tables constitute the \textit{Gini importance} or \textit{mean decrease impurity} which are defined as the total decrease in node impurity i.e. the Gini impurity in our case \cite{sklearn}. \par

\begin{table}[]
	\centering
	\begin{tabular}{r|c|c}
		& \multicolumn{2}{c}{Gini importance} \\ \midrule
		& \begin{tabular}[c]{@{}c@{}}CIC-IDS-2017\\  (max depth = 20)\end{tabular} & \begin{tabular}[c]{@{}c@{}}UNSW-NB15 \\ (max depth = 16)\end{tabular} \\ \midrule
		Source port       & 0.1007                                                                   & 0.0270                                                                \\
		Destination port  & \textbf{0.3987}                                                                   & 0.1916                                                                \\
		Protocol          & 0.0081                                                                   & 0.0043                                                                \\
		Packet Length     & 0.1279                                                                   & \textbf{0.5227}                                                                \\
		Interarrival time & 0.2409                                                                   & 0.0848                                                                \\
		Direction         & 0.0373                                                                   & 0.0083                                                                \\
		SYN Flag          & 0.0084                                                                   & 0.0057                                                                \\
		FIN Flag          & 0.0174                                                                   & 0.0005                                                                \\
		RST Flag          & 0.0261                                                                   & 0.0004                                                                \\
		PSH Flag          & 0.0234                                                                   & 0.1318                                                                \\
		ACK Flag          & 0.0110                                                                   & 0.0230                                                                \\
		URG Flag          & 0.0000                                                                   & 0.0000                                                                \\
		ECE Flag          & 0.0000                                                                   & 0.0000                                                                \\
		CWR Flag          & 0.0000                                                                   & 0.0000                                                                \\
		NS Flag           & 0.0000                                                                   & 0.0000                                                               
	\end{tabular}
	\label{table:results:explainability:feature_importance_datasets_90}
	\caption{Normalized Gini importances of features resulting from a \gls{dtc} fitted on 90\% of data from the respective dataset. Highest values are marked bold.}
\end{table}

In table \ref{table:results:explainability:feature_importance_datasets_90} the importances of the different features from both datasets are compared. The destination port and the packet length where the most valuable features for binary classification for the CIC-IDS-2017 and UNSW-NB15 datasets respectively. The URG, ECE, CWR and NS flags seem to be of no importance to classification, but that is probably due to the fact that they are rarely used. Noteworthy is that for the two datasets different features are of high importance which suggests the conclusion that they are easy to classify due to different reasons. \par

\input{results/results/rn500/dataset/dt_flows_md5_feature_importance}

\input{results/results/rn500/dataset/dt_flows15_md5_feature_importance}

In tables \ref{table:results:dtc:fimp_flows} and \ref{table:results:dtc:fimp_flows15} the normalized feature importance was calculate for trees fitted on subsets containing only benign flows and one selected attack category, processed by a \gls{dtc} or depth 5. The most important feature for classifying the respective attack category is again marked bold. These results are mostly consistent with our results from the \gls{pd} plots, even though in that case evaluation was done on flows and not packets. E.g. the high importance of \textit{Source Port} feature when classifying the \textit{SSH-Patator} attack category is reflected in the \gls{pd} plot  \ref{fig:results:lstm:pdp:cic17_source_port_ssh_patator} above. Features might change in importance when looked at in a flow context instead of only looking at single packets.

\newpage
