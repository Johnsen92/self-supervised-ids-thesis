\chapter{Results} \label{sec:results}

In this chapter, we discuss results and try to explain them based on neuron activation and \gls{pd} plots. Like with the experiments section, we will be looking at the results from \gls{lstm} and the transformer model independently.

\section{Long Short-Term Memory Model} \label{sec:results:lstm}

As a baseline, we look at results where the model has been trained in a purely supervised fashion with different amounts of data of the two datasets. The results are comparable to previous experiments with deep neural networks on these datasets \cite{fog_based_detection_survey_2020} and even slightly better in some instances. Looking at tables \ref{table:results:lstm:stats_flows_supervised}, \ref{table:results:lstm:stats_flows15_supervised} we can already see that very little supervised data is needed to achieve fairly high accuracy. For the \gls{lstm} model, going from 90\% of training data (exp. 1.1.1 and 1.2.1) to 10\% (exp. 2.1.1 and 2.4.1) only amounts to an absolute drop of 0.164\% and 0.276\% accuracy for CIC-IDS2017 and UNSW-NB15 datasets respectively. Most astounding are also the results when dropping from millions of records when training with 90\% of the datasets to just the specialized subsets containing a couple of hundred entries in total and only 10 records of each attack class. Withholding most of labeled data in the datasets, this constraint only amounts to an absolute accuracy decrease of 4.114\% and 1.176\% for datasets CIC-IDS2017 and UNSW-NB15 respectively.
While this is fairly pleasant in general, it means that results will be harder to improve as any benefit pre-training provides might be overshadowed by the effectiveness of supervised training, even with very little data. \par

\input{results/results/rn500/lstm/tables/flows_supervised/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_supervised/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_supervised/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_supervised/class_comparison_ALL}

Next, we will be looking at results for pretraining with the different proxy tasks and different amounts of data used for supervised finetuning. Tables \ref{table:results:lstm:stats_flows_10}, \ref{table:results:lstm:stats_flows_1} and \ref{table:results:lstm:stats_flows_subset} show results for experiments 2.1.1 - 2.1.6, 2.2.1 - 2.2.6 and 2.3.1 - 2.3.6 on dataset CIC-IDS2017 conducted with 10\%, 1\% and only a very small fraction of data as defined in subset CIC17\_10. Looking at the performance metrics, we can see that there is some variance in the resulting data. The NumPy and PyTorch random seeds are the same for all experiments which means that pretraining, supervised finetuning and validation have been conducted with the exact same subsets of the original dataset which means that differences in results can only come from pretraining with different proxy tasks. This establishes the fact that pretraining in general, and also different methods of pretraining have an effect on final performance. Starting with table \ref{table:results:lstm:stats_flows_10}, we can see that pretraining with some proxy tasks improves performance while others have almost no effect or even a negative effect.
For accuracy, the highest positive delta 0.101\% in experiments 2.1.1-6 in table \ref{table:results:lstm:stats_flows_10} can be observed for pretraining with the COMPOSITE proxy task \ref{sec:experiments:lstm:composite} closely followed by pretraining with the ID proxy task \ref{sec:experiments:lstm:identity} with a delta of 0.095\%. The highest negative delta in accuracy, -0.010\%, can be observed for the obscure feature proxy task \ref{sec:experiments:lstm:obscure}. It should be noted, that for detection rate, the highest delta is 0.343\% also occurring after COMPOSITE pretraining. This shows that the improvement in accuracy stems from improved attack detection capability achieved through pretraining. \par
Looking at table \ref{table:results:class_lstm:stats_flows_1} we can inspect for which attack categories improvement is most salient. When comparing training with supervised methods only in experiment 2.1.1 with the on the COMPOSITE proxy task pre-trained model in experiment 2.1.6 we can see major improvements for detection of FTP-Patator brute force attacks. Accuracy jumped from 72\% for supervised only training to 92.308\% for the COMPOSITE trained model and even 96.154\% for the PREDICT proxy task, constituting a positive delta of 24.154\%. For experiment 2.1.4, pre-training with the auto encoder task, the accuracy for detection of the FTP-Patator attack category dropped to 3.846\%. Such high variance in results shows again that the \gls{lstm} model is susceptible to different pre-training strategies. Other attack classes which have seen improvement in detection accuracy are port scans with firewall on and off (\#11-12) with positive deltas of 2.702\% and 0.606\% respectively and infiltration (\#9) and SSH-Patator (\#2) with deltas of 1.269\% and 1.185\%. \par
Looking at results for experiments 2.2.1-6 \ref{table:results:lstm:stats_flows_1} finetuned with 1\% of the CICIDS-2017 dataset - \textit{ceteris paribus} - the maximum delta in accuracy increased to 0.178\%, which in this iteration is observed after pretraining with the PREDICT proxy task \ref{sec:experiments:lstm:predict_packet}. The positive delta for the COMPOSITE proxy tasks increased to 0.136\% and the delta for the ID proxy task remained almost the same at 0.094\%. PREDICT, ID and COMPOSITE proxy tasks have shown improvements in accuracy and performance overall for experiments 2.1.1-6 and 2.2.1-6, but the pattern breaks down when looking at experiments 2.3.1-6 with finetuning performed with subset CIC17\_10 where improvement is now only present for pretraining with the ID proxy task where the positive deltas in accuracy and detection rate have increased to 0.594\% and 3.602\% respectively. All other pretraining resulted in strongly reduced performance most salient with the OBSCURE proxy task with a negative delta in accuracy of -2.327\%. \par 
It shall be noted that training with this little data entails a high variability in validation accuracy and loss over the course of training. In our case, validation accuracy was tested only every 6 epochs of training for the overall 600 training epochs to keep training times reasonable. Higher accuracy scores might have occurred during
epochs in which validation accuracy was not tested before the model started overfitting. The same caveat must be stated for experiments 2.2.1-6 in table \ref{table:results:lstm:stats_flows_1} or 2.5.1-6 in table \ref{table:results:lstm:stats_flows15_1} where validation accuracy was tested only every second epoch during training, but training for these experiments was much more stable in general and 1/2 is a much higher chance of catching the highest result than 1/6. It also shall be noted that for training with the CIC17\_10 and UNSW15\_10 the ratio of samples between categories has changed when compared to the original dataset or the stratified sampled 10\% and 1\% subsets. For CIC17\_10 and UNSW15\_10, the same amount of samples per category are included. This does not impact the comparison between pre-trained and non-pre-trained models but takes from the comparability between results of experiments 2.3.1-6 in table \ref{table:results:lstm:stats_flows_subset} and the results of experiments 2.1.1-6 in table \ref{table:results:lstm:stats_flows_10} and 2.2.1-6 in table \ref{table:results:lstm:stats_flows_1}. \par
A similar pattern emerges from the results of tests on the UNSW-NB15 dataset. For experiments 2.4.1-6 in table \ref{table:results:lstm:stats_flows15_10} finetuned with 10\% of the dataset minor improvement can again be observed for the pre-trained models. The highest positive delta of 0.086\% occurred after pre-training with de identity function proxy task in experiment 2.4.5 when comparing to the purely supvervised training in experiment 2.4.1. Training with other proxy tasks shows comparably minor improvements to accuracy. \par
Looking at class specific accuracy in table \ref{table:results:lstm:class_flows15_10} we can see that contrary to the results from experiments 2.1.1-6 on dataset CIC-IDS2017, here the increase in overall accuracy stems from minor improvements on benign flow classification i.e. an increase in specificity rather than an increase in detection rate. In fact, detection rate drops for all pre-trained models in experiments 2.4.1-6 most salient in results from experiment 2.4.6 trained on the COMPOSITE proxy-task where detection rate drops by 1.273\% but specificity increased by 0.084\% when compared to supervised only training in experiment 2.4.1 resulting in an improvement in accuracy of 0.037\% due to 96.64\% of the UNSW-NB15 dataset being benign flows. \par
The maximum delta between supervised only trained and pre-trained models increases to 0.137\% in experiments 2.5.1-6 in table \ref{table:results:lstm:stats_flows15_1}, this time occurring for proxy task AUTO with all other proxy tasks also showing minor improvements in accuracy. Just looking at experiment 2.5.4, contrary to experiments 2.4.1-6 in table  \ref{table:results:lstm:stats_flows15_10}, specificity and detection rate increased slightly when compared to baseline experiment 2.5.1. While in experiments 2.4.1-6 in table \ref{table:results:lstm:stats_flows15_10}, all pre-trained models show a decrease in detection rate, in experiments 2.5.1-6 in table  \ref{table:results:lstm:stats_flows15_1} all pre-training methods except the COMPOSITE proxy-task result in an improved detection rate. \par
The pattern of improved accuracy breaks again when looking at finetuning with the specialized subset UNSW15\_10 in table \ref{table:results:lstm:stats_flows15_subset} for experiments 2.6.1-6. Here, almost no improvement is measurable and for the COMPOSITE proxy task accuracy even dropped by 0.168\% when for finetuning with 1\% and 10\% it showed slight improvement. The only increases in accuracy are measurable for the PREDICT and AUTO proxy tasks with very minor accuracy increases of 0.005\% and 0.010\% with all other pre-training methods resulting in lower accuracy scores best represented by experiment 2.6.6 with the COMPOSITE proxy-task where accuracy dropped by 0.168\%. \par
Overall, looking at experiments with the \gls{lstm} model, no clear pattern emerges which could point at a proxy-task that generally improves accuracy when used in pre-training. Looking at table \ref{table:results:lstm:improvement_results} we could name the predict packet (PREDICT) and surprisingly the identity function (ID) proxy task as most likely to improve final accuracy of the model. The fact that any improvement was observable at all is already promising, when considering that the baseline results of exclusively supervised trained models are already very high when compared to other state-of-the-art results in contemporary research. Increasing accuracy ever so slightly without the need for more labeled data might make a \gls{ml} based \gls{ids} feasible in a real world scenario when before it was not. 

\begin{table}[h]
	\centering
	\begin{tabular}{rccccc}
		\thead{\textbf{Experiments (\#)}} & \thead{\textbf{PREDICT}} & \thead{\textbf{OBSCURE}} & \thead{\textbf{AUTO}}   & \thead{\textbf{ID}}      & \thead{\textbf{COMPOSITE}} \\ \midrule
		CIC-IDS2017 10\% (2.1.1-6) & True    & False   & False   & True    & True      \\ 
		CIC-IDS2017 1\% (2.2.1-6) & True    & False   & False   & True    & True      \\ 
		CIC-IDS2017 subset (2.3.1-6) & False   & False   & False   & True    & False     \\ 
		UNSW-NB15 10\% (2.4.1-6) & True    & True    & True    & True    & True      \\ 
		UNSW-NB15 1\% (2.5.1-6) & True    & True    & True    & True    & True      \\ 
		UNSW-NB15 subset (2.6.1-6) & True    & False   & True    & False   & False     \\ \midrule
		Results            & 83.33\% & 33.33\% & 50.00\% & 83.33\% & 66.67\%  
	\end{tabular}
	\caption{Table of comparisons whether accuracy improved for pre-trained \gls{lstm} models when compared to supervised only trained baseline experiments.}
	\label{table:results:lstm:improvement_results}
\end{table}

\input{results/results/rn500/lstm/tables/flows_10/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_10/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_1/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_1/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_subset/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_subset/class_comparison_ALL}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/lstm/plots/losses/flows_subset/training_loss.png}
	\caption{Training loss for supervised training on the \gls{lstm} model with the CIC17\_10 subset.}
	\label{fig:results:lstm:training_loss_flows_subset}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/lstm/plots/losses/flows_subset/validation_loss.png}
	\caption{Validation loss for supervised training on the \gls{lstm} model with the CIC17\_10 subset.}
	\label{fig:results:lstm:validation_loss_flows_subset}
\end{figure}

\input{results/results/rn500/lstm/tables/flows15_10/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_10/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_1/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_1/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_subset/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_subset/class_comparison_ALL}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/lstm/plots/losses/flows15_subset/training_loss.png}
	\caption{Training loss for supervised training on the \gls{lstm} model with the UNSW15\_10 subset.}
	\label{fig:results:lstm:training_loss_flows15_subset}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/lstm/plots/losses/flows15_subset/validation_loss.png}
	\caption{Validation loss for supervised training on the \gls{lstm} model with the UNSW15\_10 subset.}
	\label{fig:results:lstm:validation_loss_flows15_subset}
\end{figure}

\section{Transformer Model} \label{sec:results:transformer}

\input{results/results/rn500/transformer/tables/flows_supervised/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_supervised/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_supervised/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_supervised/class_comparison_ALL}

\begin{itemize}
	\item \ref{table:results:transformer:stats_flows_supervised} State of the art results, similar to experiments with LSTM: For 90\% of data for supervised training only 0.138\% and 0.187\% absolute difference for CIC-IDS2017 and UNSW-NB15 datasets respectively.
	\item Pretraining seems to have much less impact compared to \gls{lstm} model in general \ref{table:results:transformer:improvement_results}, but still yielded some positive results for experiments with very little supervised data (subsets) in \ref{table:results:transformer:stats_flows15_subset}
	\item For CIC-IDS2017 no improvement in accuracy could be achieved
	\item For UNSW-NB15 for 10\% and 1\% finetuning no improvements were achieved, except for a 0.03\% plus in accuracy for 3.5.4 \ref{table:results:transformer:stats_flows15_1} which is most likely just variance
	\item For experiments 3.6.1-4 \ref{table:results:transformer:stats_flows15_subset} on dataset UNSW-NB15 with subset UNSW15\_10 both the AUTO proxy task yielded minor improvements of 0.295\% accuracy but with the MASK proxy task the model accuracy fell by 1.213\% as the model defaulted to guessing negative (detection rate: 0\%).
	\item The accuracy improvements in this case stems from a major increase in detection rate of an absolute 29.424\% for experiment 3.6.4.
	\item Surprisingly, the MASK proxy task yielded the worst results overall i.e. showing the worst drops in accuracy for 4/6 experiment series even though it is the most similar proxy task used when compared to google BERT
	\item 
\end{itemize}

\begin{table}[h]
	\centering
	\begin{tabular}{rccc}
		\thead{\textbf{Experiments (\#)}} & \thead{\textbf{MASK}} & \thead{\textbf{OBSCURE}} & \thead{\textbf{AUTO}} \\ \midrule
		CIC-IDS2017 10\% (3.1.1-4)   & False  & False   & False   \\
		CIC-IDS2017 1\% (3.2.1-4)    & False  & False   & False   \\
		CIC-IDS2017 subset (3.3.1-4) & False  & False   & False   \\
		UNSW-NB15 10\% (3.4.1-4)     & False  & False   & False   \\
		UNSW-NB15 1\% (3.5.1-4)      & False  & False   & True    \\
		UNSW-NB15 subset (3.6.1-4)   & False  & False   & True    \\
		Results                      & 0.00\% & 0.00\% & 33.33\%
	\end{tabular}
	\caption{Table of comparisons whether accuracy improved for pre-trained transformer models when compared to supervised only trained baseline experiments.}
	\label{table:results:transformer:improvement_results}
\end{table}

\input{results/results/rn500/transformer/tables/flows_10/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_10/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_1/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_1/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_subset/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_subset/class_comparison_ALL}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/transformer/plots/losses/flows15_subset/training_loss.png}
	\caption{Training loss for supervised training on transformer model with the UNSW15\_10 subset.}
	\label{fig:results:transformer:training_loss_flows_subset}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/transformer/plots/losses/flows15_subset/validation_loss.png}
	\caption{Validation loss for supervised training on \gls{lstm} model with the UNSW15\_10 subset.}
	\label{fig:results:transformer:validation_loss_flows_subset}
\end{figure}

\input{results/results/rn500/transformer/tables/flows15_10/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_10/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_1/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_1/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_subset/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_subset/class_comparison_ALL}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/transformer/plots/losses/flows15_subset/training_loss.png}
	\caption{Training loss for supervised training on the transformer model with the UNSW15\_10 subset.}
	\label{fig:results:transformer:training_loss_flows15_subset}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{results/results/rn500/transformer/plots/losses/flows15_subset/validation_loss.png}
	\caption{Validation loss for supervised training on the \gls{lstm} model with the UNSW15\_10 subset.}
	\label{fig:results:transformer:validation_loss_flows15_subset}
\end{figure}


\section{Explainability}

In this section we are trying to analyze the mechanisms involved in generating the results in the previous section. As with machine learning in general, complete explainability is hard to achieve, hence the following attempts are our best efforts to make sense of the results. We try to answer the questions

\begin{itemize}
	\item Did the pre-training have any effect on the model behavior and if yes: how did predictions change?
	\item Which features where relevant for classification? 
	\item Why did the model perform so well, even with very little training data?
\end{itemize}

To answer these questions, we conducted a series of tests including plotting neuron activation data, a \gls{pd} analysis, and fitting a \gls{dtc} to discern the value thresholds of features which lead to correct classifications. 

To answer the question whether pre-training had any effect at all on the behavior of the \gls{lstm} model, we looked at neuron activations of the last stage of the \gls{lstm}, i.e. the hidden state of the last stage of the last layer of the model after having processed the whole data sequence, after pre-training and after fine-tuning. The assumption was, that if the information the model learned during pre-training proofed useful for classification the neurons which were activated after pre-training would still be activated after fine-tuning. The same neuron activations not being present after fine-tuning does however not mean, that pre-training had no positive effect on classification. As we are only looking at the last stage of the last layer of the \gls{lstm}, the learned information might have been used in a previous layer of the \gls{lstm} to influence later layers towards a more accurate classification which would not show in these results. To have some form of quantitative metric, we calculated the \gls{mae} between the activated neurons after pre-training, i.e. neurons unequal to zero, and the same neurons after fine-tuning. 

We started with the \textbf{\gls{pd}} analysis on some of the features
to discern their effect on the classification of each attack type in the dataset. 
The resulting \glspl{pdp} were generated with the models from experiments \ref{table:results:lstm:stats_flows_subset},
\ref{table:results:lstm:stats_flows15_subset}, \ref{table:results:transformer:stats_flows_subset}, \ref{table:results:transformer:stats_flows15_subset} because we assumed that pre-training has the most impact on models trained on very little labeled training data. The features we looked at where \textit{source port number}, \textit{destination port number}, \textit{protocol identifier} and \textit{packet length}.
For both datasets, we generated plots to inspect how the predictions of the model change after altering the value of the feature (\textit{ceteris paribus}). The value range of each feature was withing the highest and lowest occurring value of said feature in the dataset. After we generated the data for each tuple of $dataset x model x feature x attack type$ we generated plots to compare the results of the different models yielding a total of 160 plots. Most of them are of little interest as often the inspected feature had no or very little impact on the classification of the attack type. We will take a deeper look into this issue later in the section when we look at the structure of the decision tree used to classify each attack category. 


\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\linewidth]{results/results/rn500/lstm/plots/pdp/flows_NONE_PREDICT_OBSCURE_AUTO_ID_COMPOSITE/Destination_port_2_SSH-Patator_1.png}
	\caption{Partial dependency plot between the packet length and classification of SSH Patator attacks in the CIC-IDS2017 dataset.}
	\label{fig:results:lstm:pdp:cic17_destination_port_ssh_patator}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\linewidth]{results/results/rn500/lstm/plots/pdp/flows_NONE_PREDICT_OBSCURE_AUTO_ID_COMPOSITE/Source_port_2_SSH-Patator_0.png}
	\caption{Partial dependency plot between the source port and classification of SSH Patator attacks in the CIC-IDS2017 dataset.}
	\label{fig:results:lstm:pdp:cic17_source_port_ssh_patator}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\linewidth]{{"results/results/rn500//lstm/plots/neuron/flows_ID/latest/11_PortScan - Firewall off"}.png}
	\caption{Neuron activation plot comparing neuron activations of the latest stage of the \gls{lstm} model (after the last packet in the sequence has been processed) after pre-training with the ID proxy task and after fine-tuning with flows classified as \textit{PortScan - Firewall off} in the CIC-IDS2017 dataset.}
	\label{fig:results:lstm:neuron:cic17_id_port_scan_firewall_off}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.1\linewidth]{results/results/rn500//lstm/plots/neuron/flows_ID/latest/9_Infiltration.png}
	\caption{Neuron activation plot comparing neuron activations of the latest stage of the \gls{lstm} model (after the last packet in the sequence has been processed) after pre-training with the ID proxy task and after fine-tuning with flows classified as \textit{Infiltration} in the CIC-IDS2017 dataset.}
	\label{fig:results:lstm:neuron:cic17_id_infiltration}
\end{figure}


\begin{itemize}
	\item close look at differences in performance for different attack classes
	\item partial dependency plots
	\item neuron activation
	\item \gls{dlstm}s and transformer encoder already very effective, so improvement is hard
\end{itemize}

\newpage
