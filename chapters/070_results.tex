\chapter{Results} \label{sec:results}

In this chapter, we discuss results and try to explain them based on neuron activation and \gls{pd} plots. Like with the experiments section, we will be looking at the results from \gls{lstm} and the transformer model independently.

\section{Long Short-Term Memory Model} \label{sec:results:lstm}

As a baseline, we look at results where the model has been trained in a purely supervised fashion with different amounts of data of the two datasets. The results are comparable to previous experiments with deep neural networks on these datasets \cite{fog_based_detection_survey_2020} and even slightly better in some instances. Looking at tables \ref{table:results:lstm:stats_flows_supervised}, \ref{table:results:lstm:stats_flows15_supervised} we can already see that very little supervised data is needed to achieve fairly high accuracy. For the \gls{lstm} model, going from 90\% of training data (exp. 1.1.1 and 1.2.1) to 10\% (exp. 2.1.1 and 2.4.1) only amounts to an absolute drop of 0.164\% and 0.276\% accuracy for CIC-IDS2017 and UNSW-NB15 datasets respectively. Most astounding are also the results when dropping from millions of records when training with 90\% of the datasets to just the specialized subsets containing a couple of hundred entries in total and only 10 records of each attack class. Withholding most of labeled data in the datasets, this constraint only amounts to an absolute accuracy decrease of 4.114\% and 1.176\% for datasets CIC-IDS2017 and UNSW-NB15 respectively.
While this is fairly pleasant in general, it means that results will be harder to improve as any benefit pre-training provides might be overshadowed by the effectiveness of supervised training, even with very little data. \par

\input{results/results/rn500/lstm/tables/flows_supervised/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_supervised/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_supervised/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_supervised/class_comparison_ALL}

Next, we will be looking at results for pretraining with the different proxy tasks and different amounts of data used for supervised finetuning. Tables \ref{table:results:lstm:stats_flows_10}, \ref{table:results:lstm:stats_flows_1} and \ref{table:results:lstm:stats_flows_subset} show results for experiments 2.1.1 - 2.1.6, 2.2.1 - 2.2.6 and 2.3.1 - 2.3.6 on dataset CIC-IDS2017 conducted with 10\%, 1\% and only a very small fraction of data as defined in subset CIC17\_10. Looking at the performance metrics, we can see that there is some variance in the resulting data. The NumPy and PyTorch random seeds are the same for all experiments which means that pretraining, supervised finetuning and validation have been conducted with the exact same subsets of the original dataset which means that differences in results can only come from pretraining with different proxy tasks. This establishes the fact that pretraining in general, and also different methods of pretraining have an effect on final performance. Starting with table \ref{table:results:lstm:stats_flows_10}, we can see that pretraining with some proxy tasks improves performance while others have almost no effect or even a negative effect.  
For accuracy, the highest positive delta 0.101\% in experiments 2.1.1-6 can be observed for pretraining with the COMPOSITE proxy task \ref{sec:experiments:lstm:composite} closely followed by pretraining with the ID proxy task \ref{sec:experiments:lstm:identity} with a delta of 0.095\%. The highest negative delta in accuracy, -0.010\%, can be observed for the obscure feature proxy task \ref{sec:experiments:lstm:obscure}. It should be noted, that for detection rate, the highest delta is 0.343\% also occurring after COMPOSITE pretraining. This shows that the improvement in accuracy stems from improved attack detection capability achieved through pretraining. \par
Looking at table \ref{table:results:class_lstm:stats_flows_1} we can inspect for which attack categories improvement is most salient. When comparing training with supervised methods only in experiment 2.1.1 with the on the COMPOSITE proxy task pre-trained model in experiment 2.1.6 we can see major improvements for detection of FTP-Patator brute force attacks. Accuracy jumped from 72\% for supervised only training to 92.308\% for the COMPOSITE trained model and even 96.154\% for the PREDICT proxy task, constituting a positive delta of 24.154\%. For experiment 2.1.4, pre-training with the auto encoder task, the accuracy for detection of the FTP-Patator attack category dropped to 3.846\%. Such high variance in results shows again that the \gls{lstm} model is susceptible to different pre-training strategies. Other attack classes which have seen improvement in detection accuracy are port scans with firewall on and off (\#11-12) with positive deltas of 2.702\% and 0.606\% respectively and infiltration (\#9) and SSH-Patator (\#2) with deltas of 1.269\% and 1.185\%. \par
Looking at results for experiments 2.2.1-6 in table \ref{table:results:lstm:stats_flows_1} finetuned with 1\% of the CICIDS-2017 dataset - \textit{ceteris paribus} - the maximum delta in accuracy increased to 0.178\%, which in this iteration is observed after pretraining with the PREDICT proxy task \ref{sec:experiments:lstm:predict_packet}. The positive delta for the COMPOSITE proxy tasks increased to 0.136\% and the delta for the ID proxy task remained almost the same at 0.094\%. PREDICT, ID and COMPOSITE proxy tasks have shown improvements in accuracy and performance overall for experiments 2.1.1-6 and 2.2.1-6, but the pattern breaks down when looking at experiments 2.3.1-6 with finetuning performed with subset CIC17\_10 where improvement is now only present for pretraining with the ID proxy task where the positive deltas in accuracy and detection rate have increased to 0.594\% and 3.602\% respectively. All other pretraining resulted in strongly reduced performance most salient with the OBSCURE proxy task with a negative delta in accuracy of -2.327\%. \par 
It shall be noted that training with this little data entails a high variability in validation accuracy and loss over the course of training. In our case, validation accuracy was tested only every 6 epochs of training for the overall 600 training epochs to keep training times reasonable. Higher accuracy scores might have occurred during
epochs in which validation accuracy was not tested before the model started overfitting. The same caveat must be stated for experiments 2.2.1-6 or 2.5.1-6 where validation accuracy was tested only every second epoch during training, but training for these experiments was much more stable in general and 1/2 is a much higher chance of catching the highest result than 1/6. It also shall be noted that for training with the CIC17\_10 and UNSW15\_10 the ratio of samples between categories has changed when compared to the original dataset or the stratified sampled 10\% and 1\% subsets. For CIC17\_10 and UNSW15\_10, the same amount of samples per category are included. This does not impact the comparison between pre-trained and non-pre-trained models but takes from the comparability between results of experiments 2.3.1-6 and the results of experiments 2.1.1-6 and 2.2.1-6. \par
A similar pattern emerges from the results of tests on the UNSW-NB15 dataset. For experiments 2.4.1-6 in table \ref{table:results:lstm:stats_flows15_1} finetuned with 10\% of the dataset minor improvement can again be observed for the pre-trained models. The highest positive delta of 0.086\% occurred after pre-training with de identity function proxy task in experiment 2.4.5 when comparing to the purely supvervised training in experiment 2.4.1. Training with other proxy tasks shows comparably minor improvements to accuracy. The maximum delta between supervised only trained and pre-trained models increases to 0.137\% in experiments 2.5.1-6, this time occurring for proxy task AUTO with all other proxy tasks also showing minor improvements in accuracy. The pattern breaks again when looking at finetuning with the specialized subset UNSW15\_10 in table \ref{table:results:lstm:stats_flows15_subset} for experiments 2.6.1-6. Here, almost no improvement is measurable and for the COMPOSITE proxy task accuracy even dropped by 0.168\% when for finetuning with 1\% and 10\% it showed slight improvement. The only increase in accuracy is measurable for the AUTO proxy task with an accuracy increase of 0.01\% might well be just noise.

\input{results/results/rn500/lstm/tables/flows_10/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_10/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_1/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_1/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_subset/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows_subset/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_10/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_10/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_1/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_1/class_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_subset/stats_comparison_ALL}

\input{results/results/rn500/lstm/tables/flows15_subset/class_comparison_ALL}

\section{Transformer Model} \label{sec:results:transformer}

\input{results/results/rn500/transformer/tables/flows_supervised/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_supervised/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_supervised/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_supervised/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_10/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_10/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_1/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_1/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_subset/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows_subset/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_10/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_10/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_1/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_1/class_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_subset/stats_comparison_ALL}

\input{results/results/rn500/transformer/tables/flows15_subset/class_comparison_ALL}

\begin{itemize}
	\item maximum accuracy with 0-90-10 pre-sup-val training
	\item comparison between pretraining accuracy with different proxy tasks for 10-80-10 pre-sup-val training
	\item comparison between pretraining accuracy with different proxy tasks for 1-89-10 pre-sup-val training
	\item comparison between pretraining accuracy with different proxy tasks for subset 10\_flows subset pre-sup-val training
	\item comparison of performance improvements for different amounts of supervised training
	\item comparison of performance improvements for different compositions of pretraining data
	\item comparison between multiple datasets
	\item comparison to orthogonal initialization/random initialization/0-initialization
	\item comparison of validation loss and accuracy convergence for different pretraining tasks
	\item provide data for maximum results including class stats for both datasets to establish a feel for the maximally possible accuracy with supervised training and 90\% of data
	\item show results for different amounts of supervised data and discuss results between different proxy tasks by showing loss progression and validation accuracy over training time and comparing class stats
	\item highlight the improvement in accuracy when comparing to supervised training only
	\item look closely at differences in loss progression and validation accuracy over time between different proxy tasks
\end{itemize}

\section{Explainability}



\begin{itemize}
	\item close look at differences in performance for different attack classes
	\item partial dependency plots
	\item neuron activation
	\item \gls{dlstm}s and transformer encoder already very effective, so improvement is hard
\end{itemize}

\newpage
