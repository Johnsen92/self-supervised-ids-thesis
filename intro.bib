@article{Turing1936,
  title={On computable numbers, with an application to the Entscheidungsproblem},
  author={Turing, Alan Mathison},
  journal={J. of Math},
  volume={58},
  pages={345--363},
  year={1936}
}

@article{bert,
	author    = {Jacob Devlin and
	Ming{-}Wei Chang and
	Kenton Lee and
	Kristina Toutanova},
	title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
	Understanding},
	journal   = {CoRR},
	volume    = {abs/1810.04805},
	year      = {2018},
	url       = {http://arxiv.org/abs/1810.04805},
	archivePrefix = {arXiv},
	eprint    = {1810.04805},
	timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{attention,
	author    = {Ashish Vaswani and
	Noam Shazeer and
	Niki Parmar and
	Jakob Uszkoreit and
	Llion Jones and
	Aidan N. Gomez and
	Lukasz Kaiser and
	Illia Polosukhin},
	title     = {Attention Is All You Need},
	journal   = {CoRR},
	volume    = {abs/1706.03762},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.03762},
	archivePrefix = {arXiv},
	eprint    = {1706.03762},
	timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kitsune,
	author    = {Yisroel Mirsky and
	Tomer Doitshman and
	Yuval Elovici and
	Asaf Shabtai},
	title     = {Kitsune: An Ensemble of Autoencoders for Online Network Intrusion
	Detection},
	journal   = {CoRR},
	volume    = {abs/1802.09089},
	year      = {2018},
	url       = {http://arxiv.org/abs/1802.09089},
	archivePrefix = {arXiv},
	eprint    = {1802.09089},
	timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1802-09089.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{adversarial_recurrent_ids,
	author    = {Alexander Hartl and
	Maximilian Bachl and
	Joachim Fabini and
	Tanja Zseby},
	title     = {Explainability and Adversarial Robustness for RNNs},
	journal   = {CoRR},
	volume    = {abs/1912.09855},
	year      = {2019},
	url       = {http://arxiv.org/abs/1912.09855},
	archivePrefix = {arXiv},
	eprint    = {1912.09855},
	timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1912-09855.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{unsw_nb15,
	author = {Moustafa, Nour and Slay, Jill},
	year = {2015},
	month = {11},
	pages = {},
	title = {UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)},
	doi = {10.1109/MilCIS.2015.7348942}
}

@conference{cic_ids_2017,
	author={Iman Sharafaldin and Arash Habibi Lashkari and Ali A. Ghorbani},
	title={Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization},
	booktitle={Proceedings of the 4th International Conference on Information Systems Security and Privacy - Volume 1: ICISSP,},
	year={2018},
	pages={108-116},
	publisher={SciTePress},
	organization={INSTICC},
	doi={10.5220/0006639801080116},
	isbn={978-989-758-282-0},
}

@article{adam,
	author = {Kingma, Diederik and Ba, Jimmy},
	year = {2014},
	month = {12},
	pages = {},
	title = {Adam: A Method for Stochastic Optimization},
	journal = {International Conference on Learning Representations}
}

@article{optimizer_comparison,
	author    = {Sebastian Ruder},
	title     = {An overview of gradient descent optimization algorithms},
	journal   = {CoRR},
	volume    = {abs/1609.04747},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.04747},
	archivePrefix = {arXiv},
	eprint    = {1609.04747},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Ruder16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{autoencoders,
	title={Autoencoders}, 
	author={Dor Bank and Noam Koenigstein and Raja Giryes},
	year={2021},
	eprint={2003.05991},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{rnn_review,
	author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
	title = "{A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures}",
	journal = {Neural Computation},
	volume = {31},
	number = {7},
	pages = {1235-1270},
	year = {2019},
	month = {07},
	abstract = "{Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.}",
	issn = {0899-7667},
	doi = {10.1162/neco_a_01199},
	url = {https://doi.org/10.1162/neco\_a\_01199},
	eprint = {https://direct.mit.edu/neco/article-pdf/31/7/1235/1053200/neco\_a\_01199.pdf},
}

@article{rnn_elman,
	author = {Elman, Jeffrey L.},
	title = {Finding Structure in Time},
	journal = {Cognitive Science},
	volume = {14},
	number = {2},
	pages = {179-211},
	doi = {https://doi.org/10.1207/s15516709cog1402\_1},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
	year = {1990}
}

@article{rnn_zachary,
	author    = {Zachary Chase Lipton},
	title     = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
	journal   = {CoRR},
	volume    = {abs/1506.00019},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.00019},
	archivePrefix = {arXiv},
	eprint    = {1506.00019},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Lipton15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lstm_origin,
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	month = {12},
	pages = {1735-80},
	title = {Long Short-term Memory},
	volume = {9},
	journal = {Neural computation},
	doi = {10.1162/neco.1997.9.8.1735}
}

@article{attention_origin,
	author    = {Ashish Vaswani and
	Noam Shazeer and
	Niki Parmar and
	Jakob Uszkoreit and
	Llion Jones and
	Aidan N. Gomez and
	Lukasz Kaiser and
	Illia Polosukhin},
	title     = {Attention Is All You Need},
	journal   = {CoRR},
	volume    = {abs/1706.03762},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.03762},
	archivePrefix = {arXiv},
	eprint    = {1706.03762},
	timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@unknown{auto_encoders,
	author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
	year = {2020},
	month = {03},
	pages = {},
	title = {Autoencoders}
}

@online{tensorflow,
	author = {Google Brain Team},
	title = {TensorFlow},
	year = 2015,
	url = {https://www.tensorflow.org/},
	urldate = {2021-09-27}
}

@online{pytorch,
	author = {Adam Paszke et al.},
	title = {PyTorch},
	year = 2016,
	url = {https://pytorch.org/},
	urldate = {2021-09-27}
}

@online{arxiv,
	author = {Paul Ginsparg},
	title = {arXiv},
	year = 1991,
	url = {https://arxiv.org/},
	urldate = {2021-09-27}
}

@article{glue,
	author    = {Alex Wang and
	Amanpreet Singh and
	Julian Michael and
	Felix Hill and
	Omer Levy and
	Samuel R. Bowman},
	title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
	Language Understanding},
	journal   = {CoRR},
	volume    = {abs/1804.07461},
	year      = {2018},
	url       = {http://arxiv.org/abs/1804.07461},
	eprinttype = {arXiv},
	eprint    = {1804.07461},
	timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07461.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{books_corpus,
	author    = {Yukun Zhu and
	Ryan Kiros and
	Richard S. Zemel and
	Ruslan Salakhutdinov and
	Raquel Urtasun and
	Antonio Torralba and
	Sanja Fidler},
	title     = {Aligning Books and Movies: Towards Story-like Visual Explanations
	by Watching Movies and Reading Books},
	journal   = {CoRR},
	volume    = {abs/1506.06724},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.06724},
	eprinttype = {arXiv},
	eprint    = {1506.06724},
	timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/ZhuKZSUTF15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{wordpiece,
	author    = {Yonghui Wu and
	Mike Schuster and
	Zhifeng Chen and
	Quoc V. Le and
	Mohammad Norouzi and
	Wolfgang Macherey and
	Maxim Krikun and
	Yuan Cao and
	Qin Gao and
	Klaus Macherey and
	Jeff Klingner and
	Apurva Shah and
	Melvin Johnson and
	Xiaobing Liu and
	Lukasz Kaiser and
	Stephan Gouws and
	Yoshikiyo Kato and
	Taku Kudo and
	Hideto Kazawa and
	Keith Stevens and
	George Kurian and
	Nishant Patil and
	Wei Wang and
	Cliff Young and
	Jason Smith and
	Jason Riesa and
	Alex Rudnick and
	Oriol Vinyals and
	Greg Corrado and
	Macduff Hughes and
	Jeffrey Dean},
	title     = {Google's Neural Machine Translation System: Bridging the Gap between
	Human and Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1609.08144},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.08144},
	eprinttype = {arXiv},
	eprint    = {1609.08144},
	timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{unsupervised_learning_lstms,
	author    = {Nitish Srivastava and
	Elman Mansimov and
	Ruslan Salakhutdinov},
	title     = {Unsupervised Learning of Video Representations using LSTMs},
	journal   = {CoRR},
	volume    = {abs/1502.04681},
	year      = {2015},
	url       = {http://arxiv.org/abs/1502.04681},
	eprinttype = {arXiv},
	eprint    = {1502.04681},
	timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/SrivastavaMS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{unsupervised_learning_lstms_timeseries,
	author = {Sagheer, Alaa and Kotb, Mostafa},
	year = {2019},
	month = {12},
	pages = {19038},
	title = {Unsupervised Pre-training of a Deep LSTM-based Stacked Autoencoder for Multivariate Time Series Forecasting Problems},
	volume = {9},
	journal = {Scientific Reports},
	doi = {10.1038/s41598-019-55320-6}
}

@article{lstm_anomaly_detection,
	author    = {Lun{-}Pin Yuan and
	Peng Liu and
	Sencun Zhu},
	title     = {Recomposition vs. Prediction: {A} Novel Anomaly Detection for Discrete
	Events Based On Autoencoder},
	journal   = {CoRR},
	volume    = {abs/2012.13972},
	year      = {2020},
	url       = {https://arxiv.org/abs/2012.13972},
	eprinttype = {arXiv},
	eprint    = {2012.13972},
	timestamp = {Tue, 05 Jan 2021 16:02:31 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2012-13972.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dlstm_time_series_forecasting,
	title = {Time series forecasting of petroleum production using deep LSTM recurrent networks},
	journal = {Neurocomputing},
	volume = {323},
	pages = {203-213},
	year = {2019},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2018.09.082},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231218311639},
	author = {Alaa Sagheer and Mostafa Kotb},
	keywords = {Time series forecasting, Deep neural networks, Recurrent neural networks, Long-short term memory, Petroleum production forecasting},
	abstract = {Time series forecasting (TSF) is the task of predicting future values of a given sequence using historical data. Recently, this task has attracted the attention of researchers in the area of machine learning to address the limitations of traditional forecasting methods, which are time-consuming and full of complexity. With the increasing availability of extensive amounts of historical data along with the need of performing accurate production forecasting, particularly a powerful forecasting technique infers the stochastic dependency between past and future values is highly needed. In this paper, we propose a deep learning approach capable to address the limitations of traditional forecasting approaches and show accurate predictions. The proposed approach is a deep long-short term memory (DLSTM) architecture, as an extension of the traditional recurrent neural network. Genetic algorithm is applied in order to optimally configure DLSTM’s optimum architecture. For evaluation purpose, two case studies from the petroleum industry domain are carried out using the production data of two actual oilfields. Toward a fair evaluation, the performance of the proposed approach is compared with several standard methods, either statistical or soft computing. Using different measurement criteria, the empirical results show that the proposed DLSTM model outperforms other standard approaches.}
}

