@article{Turing1936,
  title={On computable numbers, with an application to the Entscheidungsproblem},
  author={Turing, Alan Mathison},
  journal={J. of Math},
  volume={58},
  pages={345--363},
  year={1936}
}

@article{bert,
	author    = {Jacob Devlin and
	Ming{-}Wei Chang and
	Kenton Lee and
	Kristina Toutanova},
	title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
	Understanding},
	journal   = {CoRR},
	volume    = {abs/1810.04805},
	year      = {2018},
	url       = {http://arxiv.org/abs/1810.04805},
	archivePrefix = {arXiv},
	eprint    = {1810.04805},
	timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{elmo,
	author    = {Matthew E. Peters and
	Mark Neumann and
	Mohit Iyyer and
	Matt Gardner and
	Christopher Clark and
	Kenton Lee and
	Luke Zettlemoyer},
	title     = {Deep contextualized word representations},
	journal   = {CoRR},
	volume    = {abs/1802.05365},
	year      = {2018},
	url       = {http://arxiv.org/abs/1802.05365},
	eprinttype = {arXiv},
	eprint    = {1802.05365},
	timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{attention,
	author    = {Ashish Vaswani and
	Noam Shazeer and
	Niki Parmar and
	Jakob Uszkoreit and
	Llion Jones and
	Aidan N. Gomez and
	Lukasz Kaiser and
	Illia Polosukhin},
	title     = {Attention Is All You Need},
	journal   = {CoRR},
	volume    = {abs/1706.03762},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.03762},
	archivePrefix = {arXiv},
	eprint    = {1706.03762},
	timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kitsune,
	author    = {Yisroel Mirsky and
	Tomer Doitshman and
	Yuval Elovici and
	Asaf Shabtai},
	title     = {Kitsune: An Ensemble of Autoencoders for Online Network Intrusion
	Detection},
	journal   = {CoRR},
	volume    = {abs/1802.09089},
	year      = {2018},
	url       = {http://arxiv.org/abs/1802.09089},
	archivePrefix = {arXiv},
	eprint    = {1802.09089},
	timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1802-09089.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{adversarial_recurrent_ids,
	author    = {Alexander Hartl and
	Maximilian Bachl and
	Joachim Fabini and
	Tanja Zseby},
	title     = {Explainability and Adversarial Robustness for RNNs},
	journal   = {CoRR},
	volume    = {abs/1912.09855},
	year      = {2019},
	url       = {http://arxiv.org/abs/1912.09855},
	archivePrefix = {arXiv},
	eprint    = {1912.09855},
	timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1912-09855.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{unsw_nb15,
	author = {Moustafa, Nour and Slay, Jill},
	year = {2015},
	month = {11},
	pages = {},
	title = {UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)},
	doi = {10.1109/MilCIS.2015.7348942}
}

@conference{cic_ids_2017,
	author={Iman Sharafaldin and Arash Habibi Lashkari and Ali A. Ghorbani},
	title={Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization},
	booktitle={Proceedings of the 4th International Conference on Information Systems Security and Privacy - Volume 1: ICISSP,},
	year={2018},
	pages={108-116},
	publisher={SciTePress},
	organization={INSTICC},
	doi={10.5220/0006639801080116},
	isbn={978-989-758-282-0},
}

@article{cic_ids_2017_analysis,
	author = {Panigrahi, Ranjit and Borah, Samarjeet},
	year = {2018},
	month = {01},
	pages = {479-482},
	title = {A detailed analysis of CICIDS2017 dataset for designing Intrusion Detection Systems},
	volume = {7}
}

@article{rpl_nids17,
	author = {Verma, Abhishek and Ranga, Virender},
	year = {2019},
	month = {10},
	pages = {1571–1594},
	title = {Evaluation of Network Intrusion Detection Systems for RPL Based 6LoWPAN Networks in IoT},
	volume = {108},
	journal = {Wireless Personal Communications},
	doi = {10.1007/s11277-019-06485-w}
}

@article{n_baiot,
	author    = {Yair Meidan and
	Michael Bohadana and
	Yael Mathov and
	Yisroel Mirsky and
	Dominik Breitenbacher and
	Asaf Shabtai and
	Yuval Elovici},
	title     = {N-BaIoT: Network-based Detection of IoT Botnet Attacks Using Deep
	Autoencoders},
	journal   = {CoRR},
	volume    = {abs/1805.03409},
	year      = {2018},
	url       = {http://arxiv.org/abs/1805.03409},
	eprinttype = {arXiv},
	eprint    = {1805.03409},
	timestamp = {Mon, 13 Aug 2018 16:49:04 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1805-03409.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{nsl_kdd,
	author={Tavallaee, Mahbod and Bagheri, Ebrahim and Lu, Wei and Ghorbani, Ali A.},
	booktitle={2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications}, 
	title={A detailed analysis of the KDD CUP 99 data set}, 
	year={2009},
	volume={},
	number={},
	pages={1-6},
	doi={10.1109/CISDA.2009.5356528}}

@article{adam,
	author = {Kingma, Diederik and Ba, Jimmy},
	year = {2014},
	month = {12},
	pages = {},
	title = {Adam: A Method for Stochastic Optimization},
	journal = {International Conference on Learning Representations}
}

@article{optimizer_comparison,
	author    = {Sebastian Ruder},
	title     = {An overview of gradient descent optimization algorithms},
	journal   = {CoRR},
	volume    = {abs/1609.04747},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.04747},
	archivePrefix = {arXiv},
	eprint    = {1609.04747},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Ruder16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{autoencoders,
	title={Autoencoders}, 
	author={Dor Bank and Noam Koenigstein and Raja Giryes},
	year={2021},
	eprint={2003.05991},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}


@article{rnn_review,
	author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
	title = "{A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures}",
	journal = {Neural Computation},
	volume = {31},
	number = {7},
	pages = {1235-1270},
	year = {2019},
	month = {07},
	abstract = "{Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.}",
	issn = {0899-7667},
	doi = {10.1162/neco_a_01199},
	url = {https://doi.org/10.1162/neco\_a\_01199},
	eprint = {https://direct.mit.edu/neco/article-pdf/31/7/1235/1053200/neco\_a\_01199.pdf},
}

@article{rnn_elman,
	author = {Elman, Jeffrey L.},
	title = {Finding Structure in Time},
	journal = {Cognitive Science},
	volume = {14},
	number = {2},
	pages = {179-211},
	doi = {https://doi.org/10.1207/s15516709cog1402\_1},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
	year = {1990}
}

@article{rnn_zachary,
	author    = {Zachary Chase Lipton},
	title     = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
	journal   = {CoRR},
	volume    = {abs/1506.00019},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.00019},
	archivePrefix = {arXiv},
	eprint    = {1506.00019},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Lipton15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lstm_origin,
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	month = {12},
	pages = {1735-80},
	title = {Long Short-term Memory},
	volume = {9},
	journal = {Neural computation},
	doi = {10.1162/neco.1997.9.8.1735}
}

@article{attention_origin,
	author    = {Ashish Vaswani and
	Noam Shazeer and
	Niki Parmar and
	Jakob Uszkoreit and
	Llion Jones and
	Aidan N. Gomez and
	Lukasz Kaiser and
	Illia Polosukhin},
	title     = {Attention Is All You Need},
	journal   = {CoRR},
	volume    = {abs/1706.03762},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.03762},
	archivePrefix = {arXiv},
	eprint    = {1706.03762},
	timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@unknown{auto_encoders,
	author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
	year = {2020},
	month = {03},
	pages = {},
	title = {Autoencoders}
}

@online{tensorflow,
	author = {Google Brain Team},
	title = {TensorFlow},
	year = 2015,
	url = {https://www.tensorflow.org/},
	urldate = {2021-09-27}
}

@online{pytorch,
	author = {Adam Paszke et al.},
	title = {PyTorch},
	year = 2016,
	url = {https://pytorch.org/},
	urldate = {2021-09-27}
}

@online{arxiv,
	author = {Paul Ginsparg},
	title = {arXiv},
	year = 1991,
	url = {https://arxiv.org/},
	urldate = {2021-09-27}
}

@article{glue,
	author    = {Alex Wang and
	Amanpreet Singh and
	Julian Michael and
	Felix Hill and
	Omer Levy and
	Samuel R. Bowman},
	title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
	Language Understanding},
	journal   = {CoRR},
	volume    = {abs/1804.07461},
	year      = {2018},
	url       = {http://arxiv.org/abs/1804.07461},
	eprinttype = {arXiv},
	eprint    = {1804.07461},
	timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07461.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{books_corpus,
	author    = {Yukun Zhu and
	Ryan Kiros and
	Richard S. Zemel and
	Ruslan Salakhutdinov and
	Raquel Urtasun and
	Antonio Torralba and
	Sanja Fidler},
	title     = {Aligning Books and Movies: Towards Story-like Visual Explanations
	by Watching Movies and Reading Books},
	journal   = {CoRR},
	volume    = {abs/1506.06724},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.06724},
	eprinttype = {arXiv},
	eprint    = {1506.06724},
	timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/ZhuKZSUTF15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{wordpiece,
	author    = {Yonghui Wu and
	Mike Schuster and
	Zhifeng Chen and
	Quoc V. Le and
	Mohammad Norouzi and
	Wolfgang Macherey and
	Maxim Krikun and
	Yuan Cao and
	Qin Gao and
	Klaus Macherey and
	Jeff Klingner and
	Apurva Shah and
	Melvin Johnson and
	Xiaobing Liu and
	Lukasz Kaiser and
	Stephan Gouws and
	Yoshikiyo Kato and
	Taku Kudo and
	Hideto Kazawa and
	Keith Stevens and
	George Kurian and
	Nishant Patil and
	Wei Wang and
	Cliff Young and
	Jason Smith and
	Jason Riesa and
	Alex Rudnick and
	Oriol Vinyals and
	Greg Corrado and
	Macduff Hughes and
	Jeffrey Dean},
	title     = {Google's Neural Machine Translation System: Bridging the Gap between
	Human and Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1609.08144},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.08144},
	eprinttype = {arXiv},
	eprint    = {1609.08144},
	timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{unsupervised_learning_lstms,
	author    = {Nitish Srivastava and
	Elman Mansimov and
	Ruslan Salakhutdinov},
	title     = {Unsupervised Learning of Video Representations using LSTMs},
	journal   = {CoRR},
	volume    = {abs/1502.04681},
	year      = {2015},
	url       = {http://arxiv.org/abs/1502.04681},
	eprinttype = {arXiv},
	eprint    = {1502.04681},
	timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/SrivastavaMS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{unsupervised_learning_lstms_timeseries,
	author = {Sagheer, Alaa and Kotb, Mostafa},
	year = {2019},
	month = {12},
	pages = {19038},
	title = {Unsupervised Pre-training of a Deep LSTM-based Stacked Autoencoder for Multivariate Time Series Forecasting Problems},
	volume = {9},
	journal = {Scientific Reports},
	doi = {10.1038/s41598-019-55320-6}
}

@article{lstm_anomaly_detection,
	author    = {Lun{-}Pin Yuan and
	Peng Liu and
	Sencun Zhu},
	title     = {Recomposition vs. Prediction: {A} Novel Anomaly Detection for Discrete
	Events Based On Autoencoder},
	journal   = {CoRR},
	volume    = {abs/2012.13972},
	year      = {2020},
	url       = {https://arxiv.org/abs/2012.13972},
	eprinttype = {arXiv},
	eprint    = {2012.13972},
	timestamp = {Tue, 05 Jan 2021 16:02:31 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2012-13972.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dlstm_time_series_forecasting,
	title = {Time series forecasting of petroleum production using deep LSTM recurrent networks},
	journal = {Neurocomputing},
	volume = {323},
	pages = {203-213},
	year = {2019},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2018.09.082},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231218311639},
	author = {Alaa Sagheer and Mostafa Kotb},
	keywords = {Time series forecasting, Deep neural networks, Recurrent neural networks, Long-short term memory, Petroleum production forecasting},
	abstract = {Time series forecasting (TSF) is the task of predicting future values of a given sequence using historical data. Recently, this task has attracted the attention of researchers in the area of machine learning to address the limitations of traditional forecasting methods, which are time-consuming and full of complexity. With the increasing availability of extensive amounts of historical data along with the need of performing accurate production forecasting, particularly a powerful forecasting technique infers the stochastic dependency between past and future values is highly needed. In this paper, we propose a deep learning approach capable to address the limitations of traditional forecasting approaches and show accurate predictions. The proposed approach is a deep long-short term memory (DLSTM) architecture, as an extension of the traditional recurrent neural network. Genetic algorithm is applied in order to optimally configure DLSTM’s optimum architecture. For evaluation purpose, two case studies from the petroleum industry domain are carried out using the production data of two actual oilfields. Toward a fair evaluation, the performance of the proposed approach is compared with several standard methods, either statistical or soft computing. Using different measurement criteria, the empirical results show that the proposed DLSTM model outperforms other standard approaches.}
}

@article{intrusion_detection_survey_2017,
	author = {Rajan, Vani},
	year = {2017},
	month = {10},
	pages = {375-384},
	title = {IJARCCE Towards Efficient Intrusion Detection using Deep Learning Techniques: A Review},
	volume = {6},
	doi = {10.17148/IJARCCE.2017.61066}
}

@Article{nid_ml_survey_2019,
	AUTHOR = {Liu, Hongyu and Lang, Bo},
	TITLE = {Machine Learning and Deep Learning Methods for Intrusion Detection Systems: A Survey},
	JOURNAL = {Applied Sciences},
	VOLUME = {9},
	YEAR = {2019},
	NUMBER = {20},
	ARTICLE-NUMBER = {4396},
	URL = {https://www.mdpi.com/2076-3417/9/20/4396},
	ISSN = {2076-3417},
	ABSTRACT = {Networks play important roles in modern life, and cyber security has become a vital research area. An intrusion detection system (IDS) which is an important cyber security technique, monitors the state of software and hardware running in the network. Despite decades of development, existing IDSs still face challenges in improving the detection accuracy, reducing the false alarm rate and detecting unknown attacks. To solve the above problems, many researchers have focused on developing IDSs that capitalize on machine learning methods. Machine learning methods can automatically discover the essential differences between normal data and abnormal data with high accuracy. In addition, machine learning methods have strong generalizability, so they are also able to detect unknown attacks. Deep learning is a branch of machine learning, whose performance is remarkable and has become a research hotspot. This survey proposes a taxonomy of IDS that takes data objects as the main dimension to classify and summarize machine learning-based and deep learning-based IDS literature. We believe that this type of taxonomy framework is fit for cyber security researchers. The survey first clarifies the concept and taxonomy of IDSs. Then, the machine learning algorithms frequently used in IDSs, metrics, and benchmark datasets are introduced. Next, combined with the representative literature, we take the proposed taxonomic system as a baseline and explain how to solve key IDS issues with machine learning and deep learning techniques. Finally, challenges and future developments are discussed by reviewing recent representative studies.},
	DOI = {10.3390/app9204396}
}

@article{fog_based_detection_survey_2020,
	author = {Samy, Ahmed and Yu, Haining and Zhang, Hongli},
	year = {2020},
	month = {04},
	pages = {1-1},
	title = {Fog-Based Attack Detection Framework for Internet of Things Using Deep Learning},
	volume = {PP},
	journal = {IEEE Access},
	doi = {10.1109/ACCESS.2020.2988854}
}

@article{feature_vectors,
	author = {Meghdouri, Fares and Zseby, Tanja and Iglesias Vázquez, Félix},
	year = {2018},
	month = {11},
	pages = {2196},
	title = {Analysis of Lightweight Feature Vectors for Attack Detection in Network Traffic},
	volume = {8},
	journal = {Applied Sciences},
	doi = {10.3390/app8112196}
}

@article{caia_vector,
	author = {Williams, Nigel and Zander, Sebastian and Armitage, Grenville},
	year = {2006},
	month = {10},
	pages = {5-16},
	title = {A preliminary performance comparison of five machine learning algorithms for practical IP traffic flow classification},
	volume = {36},
	journal = {Computer Communication Review},
	doi = {10.1145/1163593.1163596}
}

@article{anomaly_detection_recurrent_neural_networks,
	author    = {Benjamin J. Radford and
	Leonardo M. Apolonio and
	Antonio J. Trias and
	Jim A. Simpson},
	title     = {Network Traffic Anomaly Detection Using Recurrent Neural Networks},
	journal   = {CoRR},
	volume    = {abs/1803.10769},
	year      = {2018},
	url       = {http://arxiv.org/abs/1803.10769},
	eprinttype = {arXiv},
	eprint    = {1803.10769},
	timestamp = {Thu, 14 Oct 2021 09:17:47 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1803-10769.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
